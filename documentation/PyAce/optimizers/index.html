<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>PyAce.optimizers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>PyAce.optimizers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .Optimizer import Optimizer
from .BBB import BBB
from .FSVI import FSVI
from .HMC import HMC
from .SGLD import SGLD
from .SWAG import SWAG

from os.path import dirname, basename, isfile, join
import glob
modules = glob.glob(join(dirname(__file__), &#34;*.py&#34;))
__all__ = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(&#39;__init__.py&#39;)]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="PyAce.optimizers.hyperparameters" href="hyperparameters/index.html">PyAce.optimizers.hyperparameters</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="PyAce.optimizers.BBB"><code class="flex name class">
<span>class <span class="ident">BBB</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BBB(Optimizer):
    def __init__(self):
        super().__init__()
        self._data_iterator = None
        self._dataloader = None
        self._lr = None
        self._alpha = None
        self._base_model: tf.keras.Model = None
        self._priors_list = None
        self._weight_layers_indices = []
        self._posterior_mean_list = []
        self._posterior_std_dev_list = []
        self._priors_list = []
        self._layers_intervals = []
        self._prior2: GaussianPrior = None
        self._prior1: GaussianPrior = None
        self._prior: GaussianPrior = None
        self._step = 0



    def _guassian_likelihood(self,weights, mean, std_dev):
        &#34;&#34;&#34;
        calculates the likelihood of the weights being from a guassian distribution of the given mean and standard deviation

        Args:
            weights (tf.Tensor): the weights to test
            mean (tf.Tensor): the mean
            std_dev (tf.Tensor): the standard deviation

        Returns:
            tf.Tensor: the likelihood
        &#34;&#34;&#34;
        guassian_distribution = tfp.distributions.Normal(mean, tf.math.softplus(std_dev))
        return tf.reduce_mean(guassian_distribution.log_prob(weights))

    def _prior_guassian_likelihood(self):
        &#34;&#34;&#34;
        calculates the guassian likelihood of the weights with respect to the prior
        Returns:
            tf.Tensor: the likelihood of the weights of being sampled from the prior
        &#34;&#34;&#34;
        likelihood = 0
        mean_idx = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            for i in range(len(layer.trainable_variables)):
                likelihood += self._guassian_likelihood(
                    layer.trainable_variables[i], 
                    self._priors_list[mean_idx][i].mean(),
                    self._priors_list[mean_idx][i].stddev(),
                )
            mean_idx += 1

        return likelihood
    
    def _posterior_guassian_likelihood(self):
        &#34;&#34;&#34;
        calculates the guassian likelihood of the weights with respect to the posterior
        Returns:
            tf.Tensor: the likelihood of the weights of being sampled from the posterior
        &#34;&#34;&#34;
        likelihood = 0
        mean_idx = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            for i in range(len(layer.trainable_variables)):
                likelihood += self._guassian_likelihood(
                    layer.trainable_variables[i], 
                    self._posterior_mean_list[mean_idx][i], 
                    self._posterior_std_dev_list[mean_idx][i]
                )
            if len(layer.trainable_variables) != 0:
                mean_idx += 1

        return likelihood

    def _cost_function(self, labels: tf.Tensor, predictions: tf.Tensor):
        &#34;&#34;&#34;
        calculate the cost function as follows:
        cost = data_likelihood + alpha * kl_divergence

        Args:
            labels (tf.Tensor): the labels
            predictions (tf.Tensor): the predictions

        Returns:
            tf.Tensor: the cost
        &#34;&#34;&#34;
        posterior_likelihood = self._posterior_guassian_likelihood()
        prior_likelihood = self._prior_guassian_likelihood()
        kl_divergence = tf.math.subtract(posterior_likelihood, prior_likelihood)
        data_likelihood = self._dataset.loss()(labels, predictions)
        kl_divergence = tf.multiply(kl_divergence, self._alpha)
        return tf.math.add(data_likelihood, kl_divergence)
    


    def step(self, save_document_path = None):
        self._step += 1
        #update the weights
        noises = self._update_weights()

        # get sample and label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            # take the posterior distribution into account in the calculation of the gradients
            tape.watch(self._posterior_mean_list)
            tape.watch(self._posterior_std_dev_list)
            predictions = self._base_model(sample, training = True)
            likelihood = self._cost_function(
                label, 
                predictions
            )
        # get the weight, mean and standard deviation gradients
        weight_gradients = tape.gradient(likelihood, self._base_model.trainable_variables)
        mean_gradients = tape.gradient(likelihood, self._posterior_mean_list)
        std_dev_gradients = tape.gradient(likelihood, self._posterior_std_dev_list)

        # save the model loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(likelihood.numpy())+&#34;\n&#34;)

        new_posterior_mean_list = []
        new_posterior_std_dev_list = []
        trainable_layer_index = 0
        gradient_layer_index = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            if len(layer.trainable_variables) != 0:
                # go through weights and biases and merge their gradients into one vector
                _new_mean_layer_weights = []
                _new_std_dev_layer_weights = []

                for i in range(len(layer.trainable_variables)):
                    # calculate the new mean of the posterior
                    mean_gradient = mean_gradients[trainable_layer_index][i]+weight_gradients[gradient_layer_index]
                    _new_mean_layer_weights.append(
                        self._posterior_mean_list[trainable_layer_index][i]-self._lr*mean_gradient
                    )
                    # calculate the std_dev gradient
                    posterior_std_dev = self._posterior_std_dev_list[trainable_layer_index][i]
                    noise = noises[gradient_layer_index]
                    std_dev_grad = noise / (1 + tf.math.exp(-posterior_std_dev))
                    std_dev_grad *= weight_gradients[gradient_layer_index]
                    std_dev_grad += std_dev_gradients[trainable_layer_index][i]
                    # calculate the new standard deviation of the posterior
                    _new_std_dev_layer_weights.append(
                        posterior_std_dev-self._lr*std_dev_grad
                    )
                    gradient_layer_index += 1

                trainable_layer_index += 1
                new_posterior_mean_list.append(_new_mean_layer_weights)
                new_posterior_std_dev_list.append(_new_std_dev_layer_weights)

        #update the posteriors
        self._posterior_mean_list = new_posterior_mean_list
        self._posterior_std_dev_list = new_posterior_std_dev_list
        return likelihood






    def _update_weights(self):
        &#34;&#34;&#34;
        generate new weights following the posterior distributions
        &#34;&#34;&#34;
        noises = []
        for interval_idx in range(len(self._layers_intervals)):
            # reshape the vector and update the base model
            start = self._layers_intervals[interval_idx][0]
            end = self._layers_intervals[interval_idx][1]
            for layer_idx in range(start, end + 1):
                # go through weights and biases of the layer
                for i in range(len(self._base_model.layers[layer_idx].trainable_variables)):
                    #sample the new wights as a vector
                    w = self._base_model.layers[layer_idx].trainable_variables[i]
                    noise = tfp.distributions.Normal(
                        tf.zeros(self._posterior_mean_list[interval_idx][i].shape),
                        tf.ones(self._posterior_std_dev_list[interval_idx][i].shape)
                    ).sample()
                    noises.append(noise)
                    vector_weights = noise * tf.math.softplus(self._posterior_std_dev_list[interval_idx][i])
                    vector_weights += self._posterior_mean_list[interval_idx][i]

                    new_weights = tf.reshape(vector_weights, w.shape)
                    self._base_model.layers[layer_idx].trainable_variables[i].assign(new_weights)
        return noises



    def compile_extra_components(self, **kwargs):
        self._base_model = tf.keras.models.model_from_json(self._model_config)
        self._prior = kwargs[&#34;prior&#34;]
        if &#34;prior2&#34; in kwargs:
            self._prior2 = kwargs[&#34;prior2&#34;]
        else:  
            self._prior2 = GaussianPrior(0.0,0.0)
        self._lr = self._hyperparameters.lr
        self._pi = self._hyperparameters.pi
        self._batch_size = self._hyperparameters.batch_size
        sign = self._prior._std_dev/abs(self._prior._std_dev)
        if isinstance(self._prior._mean, int) or isinstance(self._prior._mean, float):
            self._prior = GaussianPrior(
                self._prior._mean * self._pi + self._prior2._mean * (1 - self._pi),
                sign * math.sqrt((self._prior._std_dev * self._pi)**2 + (self._prior2._std_dev * (1 - self._pi))**2),
            )

        self._alpha = self._hyperparameters.alpha
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._data_iterator = iter(self._dataloader)
        self._priors_list = self._prior.get_model_priors(self._base_model)
        self._init_BBB_arrays()


    def _init_BBB_arrays(self):
        &#34;&#34;&#34;
        initialises the posterior list, the correlated layers intervals and the trainable layer indices
        &#34;&#34;&#34;
        trainable_layer_index = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            # iterate through weights and biases of the layer
            mean_layer_posteriors = []
            std_dev_layer_posteriors = []

            if len(layer.trainable_variables) != 0:
                for i in range(len(layer.trainable_variables)):
                    mean_layer_posteriors.append(self._priors_list[trainable_layer_index][i].mean())
                    std_dev_layer_posteriors.append(self._priors_list[trainable_layer_index][i].stddev())
                self._weight_layers_indices.append(layer_idx)
                self._layers_intervals.append([layer_idx, layer_idx])
                self._posterior_mean_list.append(mean_layer_posteriors)
                self._posterior_std_dev_list.append(std_dev_layer_posteriors)
            trainable_layer_index += 1



    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for layer_mean_list, layer_std_dev_list, idx in zip(self._posterior_mean_list, self._posterior_std_dev_list, range(len(self._weight_layers_indices))):
            for i in range(len(layer_mean_list)):
                layer_mean_list[i] = tf.reshape(layer_mean_list[i], (-1,))
                layer_std_dev_list[i]= tf.reshape(layer_std_dev_list[i], (-1,))
            mean = tf.concat(layer_mean_list, 0)
            std_dev = tf.concat(layer_std_dev_list,0)
            tf.debugging.check_numerics(mean, &#34;mean&#34;)
            tf.debugging.check_numerics(std_dev, &#34;standard deviation&#34;)
            tf_dist = tfp.distributions.Normal(
                tf.reshape(mean, (-1,)),
                tf.math.softplus(tf.reshape(std_dev, (-1,)))
            )
            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )

            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]
            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.BBB.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._base_model = tf.keras.models.model_from_json(self._model_config)
    self._prior = kwargs[&#34;prior&#34;]
    if &#34;prior2&#34; in kwargs:
        self._prior2 = kwargs[&#34;prior2&#34;]
    else:  
        self._prior2 = GaussianPrior(0.0,0.0)
    self._lr = self._hyperparameters.lr
    self._pi = self._hyperparameters.pi
    self._batch_size = self._hyperparameters.batch_size
    sign = self._prior._std_dev/abs(self._prior._std_dev)
    if isinstance(self._prior._mean, int) or isinstance(self._prior._mean, float):
        self._prior = GaussianPrior(
            self._prior._mean * self._pi + self._prior2._mean * (1 - self._pi),
            sign * math.sqrt((self._prior._std_dev * self._pi)**2 + (self._prior2._std_dev * (1 - self._pi))**2),
        )

    self._alpha = self._hyperparameters.alpha
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._data_iterator = iter(self._dataloader)
    self._priors_list = self._prior.get_model_priors(self._base_model)
    self._init_BBB_arrays()</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.BBB.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for layer_mean_list, layer_std_dev_list, idx in zip(self._posterior_mean_list, self._posterior_std_dev_list, range(len(self._weight_layers_indices))):
        for i in range(len(layer_mean_list)):
            layer_mean_list[i] = tf.reshape(layer_mean_list[i], (-1,))
            layer_std_dev_list[i]= tf.reshape(layer_std_dev_list[i], (-1,))
        mean = tf.concat(layer_mean_list, 0)
        std_dev = tf.concat(layer_std_dev_list,0)
        tf.debugging.check_numerics(mean, &#34;mean&#34;)
        tf.debugging.check_numerics(std_dev, &#34;standard deviation&#34;)
        tf_dist = tfp.distributions.Normal(
            tf.reshape(mean, (-1,)),
            tf.math.softplus(tf.reshape(std_dev, (-1,)))
        )
        tf_dist = TensorflowProbabilityDistribution(
            tf_dist
        )

        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]
        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.BBB.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    self._step += 1
    #update the weights
    noises = self._update_weights()

    # get sample and label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        # take the posterior distribution into account in the calculation of the gradients
        tape.watch(self._posterior_mean_list)
        tape.watch(self._posterior_std_dev_list)
        predictions = self._base_model(sample, training = True)
        likelihood = self._cost_function(
            label, 
            predictions
        )
    # get the weight, mean and standard deviation gradients
    weight_gradients = tape.gradient(likelihood, self._base_model.trainable_variables)
    mean_gradients = tape.gradient(likelihood, self._posterior_mean_list)
    std_dev_gradients = tape.gradient(likelihood, self._posterior_std_dev_list)

    # save the model loss if the path is specified
    if save_document_path != None:
        with open(save_document_path, &#34;a&#34;) as losses_file:
            losses_file.write(str(likelihood.numpy())+&#34;\n&#34;)

    new_posterior_mean_list = []
    new_posterior_std_dev_list = []
    trainable_layer_index = 0
    gradient_layer_index = 0
    for layer_idx in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_idx]
        if len(layer.trainable_variables) != 0:
            # go through weights and biases and merge their gradients into one vector
            _new_mean_layer_weights = []
            _new_std_dev_layer_weights = []

            for i in range(len(layer.trainable_variables)):
                # calculate the new mean of the posterior
                mean_gradient = mean_gradients[trainable_layer_index][i]+weight_gradients[gradient_layer_index]
                _new_mean_layer_weights.append(
                    self._posterior_mean_list[trainable_layer_index][i]-self._lr*mean_gradient
                )
                # calculate the std_dev gradient
                posterior_std_dev = self._posterior_std_dev_list[trainable_layer_index][i]
                noise = noises[gradient_layer_index]
                std_dev_grad = noise / (1 + tf.math.exp(-posterior_std_dev))
                std_dev_grad *= weight_gradients[gradient_layer_index]
                std_dev_grad += std_dev_gradients[trainable_layer_index][i]
                # calculate the new standard deviation of the posterior
                _new_std_dev_layer_weights.append(
                    posterior_std_dev-self._lr*std_dev_grad
                )
                gradient_layer_index += 1

            trainable_layer_index += 1
            new_posterior_mean_list.append(_new_mean_layer_weights)
            new_posterior_std_dev_list.append(_new_std_dev_layer_weights)

    #update the posteriors
    self._posterior_mean_list = new_posterior_mean_list
    self._posterior_std_dev_list = new_posterior_std_dev_list
    return likelihood</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.BBB.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.FSVI"><code class="flex name class">
<span>class <span class="ident">FSVI</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class FSVI(Optimizer):
    def __init__(self):
        super().__init__()
        self._data_iterator = None
        self._dataloader = None
        self._layers_dtbn = []

    def predict_region(self, predict_data: tf.Tensor):
        x_mean = tf.math.reduce_mean(predict_data, axis=0)
        x_std = tf.math.reduce_std()
        self._predict_dtbn = tfp.distributions.Normal(x_mean, x_std)

    def sample_training(self):
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))
        return sample
    
    def sample_predicts(self, pnum):
        pred_samples = []
        for n in range(pnum):
            s = self._predict_dtbn.sample()
            pred_samples.append(s)
        xs = tf.convert_to_tensor(pred_samples)
        return tf.reshape(xs, shape=(1,-1)) 
    
    def vi_noise(self):
        layers_weights = self._vi_model.layers.get_weights()
        for layer_w in layers_weights:
            w_mean = tf.fill(layer_w.shape, 0)
            w_std = tf.fill(layer_w.shape, self._vi_std)
            dtbn = tfp.distributions.Normal(w_mean, w_std) 
            self._layers_dtbn.append(dtbn)

    def vi_fwd_noise(self, t_samples, p_samples):
        layers_weights = self._vi_model.layers.get_weights()
        vi_weights = []
        for l in range(len(self._layers_dtbn)):
            layer_noise = self._layers_dtbn[l].sample()
            vi_w = tf.math.add(layers_weights[l], layer_noise)
            vi_weights.append(vi_w)
        self._vi_model.set_weights(vi_weights)

        t_funcs = self._vi_model(t_samples, training=True)
        p_funcs = self._vi_model(p_samples, training=True)
        return t_funcs, p_funcs
    
    def step(self, save_document_path=None):
        
        t_samples = self.sample_training()
        pnum = self._hyperparameters.pred_num
        p_samples = self.sample_predicts(pnum)
        for i in range(self._k_vifunc):
            t_funcs, p_funcs = self.vi_fwd_noise(t_samples, p_samples)
        
        
        return super().step(save_document_path)


    def compile_extra_components(self, **kwargs):
        self._vi_model = tf.keras.models.model_from_json(self._model_config)
        self._noise_std = self._hyperparameters.noise_std
        self._vi_std = self._hyperparameters.vi_std
        self._k_vifunc = self._hyperparameters.k_vifunc
        self._prior = kwargs[&#34;prior&#34;]
        self._lr = self._hyperparameters.lr
        self._alpha = self._hyperparameters.alpha
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(128))
        self._data_iterator = iter(self._dataloader)
        self._priors_list = self._prior.get_model_priors(self._base_model)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.FSVI.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._vi_model = tf.keras.models.model_from_json(self._model_config)
    self._noise_std = self._hyperparameters.noise_std
    self._vi_std = self._hyperparameters.vi_std
    self._k_vifunc = self._hyperparameters.k_vifunc
    self._prior = kwargs[&#34;prior&#34;]
    self._lr = self._hyperparameters.lr
    self._alpha = self._hyperparameters.alpha
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(128))
    self._data_iterator = iter(self._dataloader)
    self._priors_list = self._prior.get_model_priors(self._base_model)</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.predict_region"><code class="name flex">
<span>def <span class="ident">predict_region</span></span>(<span>self, predict_data: tensorflow.python.framework.tensor.Tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def predict_region(self, predict_data: tf.Tensor):
    x_mean = tf.math.reduce_mean(predict_data, axis=0)
    x_std = tf.math.reduce_std()
    self._predict_dtbn = tfp.distributions.Normal(x_mean, x_std)</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.sample_predicts"><code class="name flex">
<span>def <span class="ident">sample_predicts</span></span>(<span>self, pnum)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_predicts(self, pnum):
    pred_samples = []
    for n in range(pnum):
        s = self._predict_dtbn.sample()
        pred_samples.append(s)
    xs = tf.convert_to_tensor(pred_samples)
    return tf.reshape(xs, shape=(1,-1)) </code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.sample_training"><code class="name flex">
<span>def <span class="ident">sample_training</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_training(self):
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))
    return sample</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path=None):
    
    t_samples = self.sample_training()
    pnum = self._hyperparameters.pred_num
    p_samples = self.sample_predicts(pnum)
    for i in range(self._k_vifunc):
        t_funcs, p_funcs = self.vi_fwd_noise(t_samples, p_samples)
    
    
    return super().step(save_document_path)</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.vi_fwd_noise"><code class="name flex">
<span>def <span class="ident">vi_fwd_noise</span></span>(<span>self, t_samples, p_samples)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vi_fwd_noise(self, t_samples, p_samples):
    layers_weights = self._vi_model.layers.get_weights()
    vi_weights = []
    for l in range(len(self._layers_dtbn)):
        layer_noise = self._layers_dtbn[l].sample()
        vi_w = tf.math.add(layers_weights[l], layer_noise)
        vi_weights.append(vi_w)
    self._vi_model.set_weights(vi_weights)

    t_funcs = self._vi_model(t_samples, training=True)
    p_funcs = self._vi_model(p_samples, training=True)
    return t_funcs, p_funcs</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.FSVI.vi_noise"><code class="name flex">
<span>def <span class="ident">vi_noise</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def vi_noise(self):
    layers_weights = self._vi_model.layers.get_weights()
    for layer_w in layers_weights:
        w_mean = tf.fill(layer_w.shape, 0)
        w_std = tf.fill(layer_w.shape, self._vi_std)
        dtbn = tfp.distributions.Normal(w_mean, w_std) 
        self._layers_dtbn.append(dtbn)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.HMC"><code class="flex name class">
<span>class <span class="ident">HMC</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HMC(Optimizer):

    def __init__(self):
        super().__init__()
        self._nb_burn_epoch = 10
        self._X = None
        self._y = None
        self._training_dataset_cardinality = None
        self._burn_epochs = None
        self._training_dataset = None
        self._prior = None
        self._p = None
        self._frequency = None
        self._samples = None
        self._model = None
        self._epsilon = None
        self._L = None
        self._m = None
        self._total_runs = 0
        self._accepted_runs = 0
        self._current_loss = 0

    def compile_extra_components(self, **kwargs):
        self._m = self._hyperparameters.m
        self._L = self._hyperparameters.L
        self._epsilon = self._hyperparameters.epsilon
        self._model: tf.keras.models.Model = tf.keras.models.model_from_json(self._model_config)
        self._samples = []
        self._frequency = []
        self._p = []
        self._prior = kwargs[&#34;prior&#34;].get_model_priors(self._model)
        if &#34;nb_burn_epoch&#34; in kwargs:
            self._nb_burn_epoch = kwargs[&#34;nb_burn_epochs&#34;]
        self._training_dataset: tf.data.Dataset = self._dataset.training_dataset()
        self._training_dataset_cardinality = self._training_dataset.cardinality().numpy().item()
        self._X, self._y = next(iter(self._training_dataset.batch(self._training_dataset.cardinality())))
        for layer in self._model.layers:
            self._p.append([tf.Variable(tf.zeros(w.shape)) for w in layer.trainable_variables])

        for layer, distribs in zip(self._model.layers, self._prior):
            if len(layer.trainable_variables) &gt; 0:
                for w, d in zip(layer.trainable_variables, distribs):
                    w.assign(d.mean())

    def step(self, save_document_path=None, sampling=True, burning=False):
        if len(self._frequency) == 0 and sampling:
            self._frequency.append(1)
            self._samples.append(self._snapshot_q())
        self._sample_kinetic_energy()
        current_k = self._kinetic_energy()
        current_u, current_loss = self._potential_energy()
        current_q = self._snapshot_q()
        self._step_p(self._epsilon / 2)
        for i in range(self._L):
            self._step_q(self._epsilon)
            if i != self._L:
                self._step_p(self._epsilon)
        self._step_p(self._epsilon / 2)
        new_k = self._kinetic_energy()
        new_u, new_loss = self._potential_energy()
        self._total_runs += 1
        if burning or random.random() &lt; tf.math.exp(current_k + current_u - new_k - new_u).numpy().item():
            self._accepted_runs += 1
            if sampling:
                self._frequency.append(1)
                self._samples.append(self._snapshot_q())
            return new_loss
        else:
            # if rejected restore old layers variables
            for layer, current_layer in zip(self._model.layers, current_q):
                for w, q in zip(layer.trainable_variables, current_layer):
                    w.assign(q)
            if sampling:
                self._frequency[len(self._frequency) - 1] += 1
            return current_loss

    def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
              model_save_path: str = None):
        self._accepted_runs = 0
        self._total_runs = 0
        nb_burn_epoch = self._nb_burn_epoch
        for i in range(nb_burn_epoch):
            loss = self.step(sampling=False, burning=True)
            accept_rate = self._accepted_runs / self._total_runs
            self._print_progress((i + 1) / nb_burn_epoch, suffix=&#34;HMC - Burning&#34;, loss=loss.numpy().item(),
                                 accept_rate=accept_rate, bar_length=20)
        self._new_progress_line()
        self._accepted_runs = 0
        self._total_runs = 0
        self._frequency = []
        self._samples = []
        for i in range(nb_iterations):
            loss = self.step(sampling=True, burning=False)
            accept_rate = self._accepted_runs / self._total_runs
            self._print_progress((i + 1) / nb_iterations, suffix=&#34;HMC - Sampling&#34;, loss=loss.numpy().item(),
                                 accept_rate=accept_rate, bar_length=20)
        self._new_progress_line()

    def _step_p(self, step_size):
        with tf.GradientTape(persistent=True) as tape:
            tape.watch(self._model.trainable_variables)
            U, _ = self._potential_energy()
        for layer, momentum_layer in zip(self._model.layers, self._p):
            for q, p in zip(layer.trainable_variables, momentum_layer):
                q_grad = tape.gradient(U, q)
                p.assign_sub(tf.multiply(q_grad, step_size))
        del tape  # free the gradient resources

    def _step_q(self, step_size):
        for layer, momentum_layer in zip(self._model.layers, self._p):
            for q, p in zip(layer.trainable_variables, momentum_layer):
                q.assign_add(tf.multiply(p, step_size / self._m))

    def _snapshot_q(self):
        q = []
        for layer in self._model.layers:
            q.append([tf.identity(w) for w in layer.trainable_variables])
        return q

    def _potential_energy(self) -&gt; (tf.Tensor, tf.Tensor):
        potential_energy = tf.constant([0.0])
        for layer, distribs in zip(self._model.layers, self._prior):
            if len(layer.trainable_variables) &gt; 0:
                for w, d in zip(layer.trainable_variables, distribs):
                    potential_energy -= tf.math.reduce_sum(d.log_prob(w))
        predictions = self._model(self._X)
        # the loss is already the log likelihood of the data in this case
        loss = self._dataset.loss()(self._y, predictions)
        potential_energy += loss * self._training_dataset_cardinality
        return potential_energy, loss

    def _kinetic_energy(self):
        kinetic_energy = tf.constant([0.0])
        for layer in self._p:
            for var_p in layer:
                kinetic_energy += (1 / (2 * self._m)) * tf.math.reduce_sum(tf.square(var_p))
        return kinetic_energy

    def _sample_kinetic_energy(self):
        for layer in self._p:
            for w in layer:
                w.assign(tf.random.normal(w.shape, stddev=self._m, mean=0))

    def update_parameters_step(self):
        pass

    def result(self) -&gt; BayesianModel:
        samples_unrolled = []
        for sample in self._samples:
            concat_unrolled = []
            for sample_layer in sample:
                for w in sample_layer:
                    concat_unrolled.append(tf.reshape(w, (-1,)))
            samples_unrolled.append(tf.concat(concat_unrolled, axis=0))
        distribution = Sampled(samples_unrolled, self._frequency)
        posterior_model = BayesianModel(self._model_config)
        posterior_model.apply_distribution(distribution, 0, len(self._model.layers) - 1)
        return posterior_model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.HMC.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._m = self._hyperparameters.m
    self._L = self._hyperparameters.L
    self._epsilon = self._hyperparameters.epsilon
    self._model: tf.keras.models.Model = tf.keras.models.model_from_json(self._model_config)
    self._samples = []
    self._frequency = []
    self._p = []
    self._prior = kwargs[&#34;prior&#34;].get_model_priors(self._model)
    if &#34;nb_burn_epoch&#34; in kwargs:
        self._nb_burn_epoch = kwargs[&#34;nb_burn_epochs&#34;]
    self._training_dataset: tf.data.Dataset = self._dataset.training_dataset()
    self._training_dataset_cardinality = self._training_dataset.cardinality().numpy().item()
    self._X, self._y = next(iter(self._training_dataset.batch(self._training_dataset.cardinality())))
    for layer in self._model.layers:
        self._p.append([tf.Variable(tf.zeros(w.shape)) for w in layer.trainable_variables])

    for layer, distribs in zip(self._model.layers, self._prior):
        if len(layer.trainable_variables) &gt; 0:
            for w, d in zip(layer.trainable_variables, distribs):
                w.assign(d.mean())</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.HMC.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    samples_unrolled = []
    for sample in self._samples:
        concat_unrolled = []
        for sample_layer in sample:
            for w in sample_layer:
                concat_unrolled.append(tf.reshape(w, (-1,)))
        samples_unrolled.append(tf.concat(concat_unrolled, axis=0))
    distribution = Sampled(samples_unrolled, self._frequency)
    posterior_model = BayesianModel(self._model_config)
    posterior_model.apply_distribution(distribution, 0, len(self._model.layers) - 1)
    return posterior_model</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.HMC.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None, sampling=True, burning=False)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path=None, sampling=True, burning=False):
    if len(self._frequency) == 0 and sampling:
        self._frequency.append(1)
        self._samples.append(self._snapshot_q())
    self._sample_kinetic_energy()
    current_k = self._kinetic_energy()
    current_u, current_loss = self._potential_energy()
    current_q = self._snapshot_q()
    self._step_p(self._epsilon / 2)
    for i in range(self._L):
        self._step_q(self._epsilon)
        if i != self._L:
            self._step_p(self._epsilon)
    self._step_p(self._epsilon / 2)
    new_k = self._kinetic_energy()
    new_u, new_loss = self._potential_energy()
    self._total_runs += 1
    if burning or random.random() &lt; tf.math.exp(current_k + current_u - new_k - new_u).numpy().item():
        self._accepted_runs += 1
        if sampling:
            self._frequency.append(1)
            self._samples.append(self._snapshot_q())
        return new_loss
    else:
        # if rejected restore old layers variables
        for layer, current_layer in zip(self._model.layers, current_q):
            for w, q in zip(layer.trainable_variables, current_layer):
                w.assign(q)
        if sampling:
            self._frequency[len(self._frequency) - 1] += 1
        return current_loss</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.HMC.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None, model_save_path: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>trains the model and saved the training metrics and model status</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nb_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>number of training iterations</dd>
<dt><strong><code>loss_save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the loss during training. Defaults to None.</dd>
<dt><strong><code>model_save_frequency</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The frequency of saving the models during training. Defaults to None.</dd>
<dt><strong><code>model_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the models during training. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>if the model saving path is specified and the frequency of saving the model is not, or</dd>
</dl>
<p>if the frequency of saving the model is sprecified and the model saving path is not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
          model_save_path: str = None):
    self._accepted_runs = 0
    self._total_runs = 0
    nb_burn_epoch = self._nb_burn_epoch
    for i in range(nb_burn_epoch):
        loss = self.step(sampling=False, burning=True)
        accept_rate = self._accepted_runs / self._total_runs
        self._print_progress((i + 1) / nb_burn_epoch, suffix=&#34;HMC - Burning&#34;, loss=loss.numpy().item(),
                             accept_rate=accept_rate, bar_length=20)
    self._new_progress_line()
    self._accepted_runs = 0
    self._total_runs = 0
    self._frequency = []
    self._samples = []
    for i in range(nb_iterations):
        loss = self.step(sampling=True, burning=False)
        accept_rate = self._accepted_runs / self._total_runs
        self._print_progress((i + 1) / nb_iterations, suffix=&#34;HMC - Sampling&#34;, loss=loss.numpy().item(),
                             accept_rate=accept_rate, bar_length=20)
    self._new_progress_line()</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.HMC.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.Optimizer"><code class="flex name class">
<span>class <span class="ident">Optimizer</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Optimizer(ABC):

    def __init__(self):
        self._model_config = None
        self._hyperparameters = None
        self.__compiled = False
        self._dataset: Dataset = None

    @abstractmethod
    def step(self, save_document_path=None):
        &#34;&#34;&#34;
        TODO : Add loss return signature
        Performs one step of the training

        Args:
            save_document_path (_type_, optional): The path to save the losses during the training. Defaults to None.

        Returns:
            float: the loss value after the step
        &#34;&#34;&#34;
        pass

    def compile(self, hyperparameters: HyperParameters, model_config: str, dataset, verbose=True,**kwargs):
        &#34;&#34;&#34;compile the model

        Args:
            hyperparameters (HyperParameters): the model hyperparameters
            model_config (dict): the configuration of the model
            dataset (_type_): the dataset of the model

        Raises:
            Exception: raises error if the model is already compiled
        &#34;&#34;&#34;
        if self.__compiled:
            raise Exception(&#34;Model Already compiled&#34;)
        else:
            self.__compiled = True
            self._hyperparameters = hyperparameters
            self._model_config = model_config
            self._dataset = dataset
            self._verbose = verbose
        self.compile_extra_components(**kwargs)

    @abstractmethod
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
        used to compile components of subclasses
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def update_parameters_step(self):
        &#34;&#34;&#34;
        one step of updating the model parameters
        &#34;&#34;&#34;
        pass

    def _empty_folder(self, path):
        for filename in os.listdir(path):
            file_path = os.path.join(path, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(&#39;Failed to delete %s. Reason: %s&#39; % (file_path, e))

    def train_with_weights_and_biases(self, nb_iterations, project_name, weights_and_biases_config):
        wandb.login()
        run = wandb.init(project= project_name, config=weights_and_biases_config)
        self.train(nb_iterations, weights_and_biases_log = True)

    def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
              model_save_path: str = None, weights_and_biases_log = False):
        &#34;&#34;&#34;
        trains the model and saved the training metrics and model status

        Args:
            nb_iterations (int): number of training iterations
            loss_save_document_path (str, optional): The path to save the loss during training. Defaults to None.
            model_save_frequency (int, optional): The frequency of saving the models during training. Defaults to None.
            model_save_path (str, optional): The path to save the models during training. Defaults to None.

        Raises:
            Exception: if the model saving path is specified and the frequency of saving the model is not, or
            if the frequency of saving the model is sprecified and the model saving path is not.
        &#34;&#34;&#34;
        if model_save_frequency == None and model_save_path != None:
            raise Exception(&#34;Error: save path precised and save frequency is None, please provide a savong frequency&#34;)
        if model_save_frequency != None and model_save_path == None:
            raise Exception(&#34;Error: save frequency precised and save path is None, please provide a saving path&#34;)

        if loss_save_document_path != None and os.path.exists(loss_save_document_path):
            os.remove(loss_save_document_path)

        if model_save_path != None:
            self._empty_folder(model_save_path)            


        saved_model_nbr = 0
        for i in range(nb_iterations):
            loss = self.step(loss_save_document_path)
            self._print_progress(i / nb_iterations, loss=loss)
            if weights_and_biases_log == True:
                wandb.log({
                    &#34;loss&#34;: loss
                })
            if model_save_frequency != None and i % model_save_frequency == 0:
                bayesian_model = self.result()
                if os.path.exists(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr))):
                    shutil.rmtree(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                os.makedirs(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                bayesian_model.store(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                saved_model_nbr += 1

        print()

    @abstractmethod
    def result(self) -&gt; BayesianModel:
        &#34;&#34;&#34;
        create a bayesian model at the stage of the training

        Returns:
            BayesianModel: the bayesian model trained
        &#34;&#34;&#34;
        pass

    def _print_progress(self, progress: float, bar_length=10, suffix=&#34;Training&#34;, **kwargs):
        if not self._verbose:
            return
        nb_chars = math.ceil(progress * bar_length)
        bar = &#34;[&#34; + nb_chars * &#34;=&#34;
        if nb_chars &lt; bar_length:
            bar += &#34;&gt;&#34;
        bar += &#34;]&#34;
        infos = &#39; &#39;.join(&#34;{}: {}&#34;.format(k, v) for k, v in kwargs.items())
        percentage = str(math.ceil(progress * 100))
        print(&#34;\r&#34; + suffix + &#34; &#34; + percentage + &#34; % &#34; + bar + &#34; &#34; + infos, end=&#34;&#34;)

    def _new_progress_line(self):
        if not self._verbose:
            return
        print()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>PyAce.optimizers.BBB.BBB</li>
<li>PyAce.optimizers.FSVI.FSVI</li>
<li>PyAce.optimizers.HMC.HMC</li>
<li>PyAce.optimizers.SGLD.SGLD</li>
<li>PyAce.optimizers.SWAG.SWAG</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.Optimizer.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, hyperparameters: <a title="PyAce.optimizers.hyperparameters.HyperParameters.HyperParameters" href="hyperparameters/HyperParameters.html#PyAce.optimizers.hyperparameters.HyperParameters.HyperParameters">HyperParameters</a>, model_config: str, dataset, verbose=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compile the model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>hyperparameters</code></strong> :&ensp;<code>HyperParameters</code></dt>
<dd>the model hyperparameters</dd>
<dt><strong><code>model_config</code></strong> :&ensp;<code>dict</code></dt>
<dd>the configuration of the model</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>_type_</code></dt>
<dd>the dataset of the model</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>raises error if the model is already compiled</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self, hyperparameters: HyperParameters, model_config: str, dataset, verbose=True,**kwargs):
    &#34;&#34;&#34;compile the model

    Args:
        hyperparameters (HyperParameters): the model hyperparameters
        model_config (dict): the configuration of the model
        dataset (_type_): the dataset of the model

    Raises:
        Exception: raises error if the model is already compiled
    &#34;&#34;&#34;
    if self.__compiled:
        raise Exception(&#34;Model Already compiled&#34;)
    else:
        self.__compiled = True
        self._hyperparameters = hyperparameters
        self._model_config = model_config
        self._dataset = dataset
        self._verbose = verbose
    self.compile_extra_components(**kwargs)</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
    used to compile components of subclasses
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def result(self) -&gt; BayesianModel:
    &#34;&#34;&#34;
    create a bayesian model at the stage of the training

    Returns:
        BayesianModel: the bayesian model trained
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def step(self, save_document_path=None):
    &#34;&#34;&#34;
    TODO : Add loss return signature
    Performs one step of the training

    Args:
        save_document_path (_type_, optional): The path to save the losses during the training. Defaults to None.

    Returns:
        float: the loss value after the step
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None, model_save_path: str = None, weights_and_biases_log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>trains the model and saved the training metrics and model status</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nb_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>number of training iterations</dd>
<dt><strong><code>loss_save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the loss during training. Defaults to None.</dd>
<dt><strong><code>model_save_frequency</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The frequency of saving the models during training. Defaults to None.</dd>
<dt><strong><code>model_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the models during training. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>if the model saving path is specified and the frequency of saving the model is not, or</dd>
</dl>
<p>if the frequency of saving the model is sprecified and the model saving path is not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
          model_save_path: str = None, weights_and_biases_log = False):
    &#34;&#34;&#34;
    trains the model and saved the training metrics and model status

    Args:
        nb_iterations (int): number of training iterations
        loss_save_document_path (str, optional): The path to save the loss during training. Defaults to None.
        model_save_frequency (int, optional): The frequency of saving the models during training. Defaults to None.
        model_save_path (str, optional): The path to save the models during training. Defaults to None.

    Raises:
        Exception: if the model saving path is specified and the frequency of saving the model is not, or
        if the frequency of saving the model is sprecified and the model saving path is not.
    &#34;&#34;&#34;
    if model_save_frequency == None and model_save_path != None:
        raise Exception(&#34;Error: save path precised and save frequency is None, please provide a savong frequency&#34;)
    if model_save_frequency != None and model_save_path == None:
        raise Exception(&#34;Error: save frequency precised and save path is None, please provide a saving path&#34;)

    if loss_save_document_path != None and os.path.exists(loss_save_document_path):
        os.remove(loss_save_document_path)

    if model_save_path != None:
        self._empty_folder(model_save_path)            


    saved_model_nbr = 0
    for i in range(nb_iterations):
        loss = self.step(loss_save_document_path)
        self._print_progress(i / nb_iterations, loss=loss)
        if weights_and_biases_log == True:
            wandb.log({
                &#34;loss&#34;: loss
            })
        if model_save_frequency != None and i % model_save_frequency == 0:
            bayesian_model = self.result()
            if os.path.exists(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr))):
                shutil.rmtree(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            os.makedirs(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            bayesian_model.store(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            saved_model_nbr += 1

    print()</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.train_with_weights_and_biases"><code class="name flex">
<span>def <span class="ident">train_with_weights_and_biases</span></span>(<span>self, nb_iterations, project_name, weights_and_biases_config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_with_weights_and_biases(self, nb_iterations, project_name, weights_and_biases_config):
    wandb.login()
    run = wandb.init(project= project_name, config=weights_and_biases_config)
    self.train(nb_iterations, weights_and_biases_log = True)</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.Optimizer.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def update_parameters_step(self):
    &#34;&#34;&#34;
    one step of updating the model parameters
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.SGLD"><code class="flex name class">
<span>class <span class="ident">SGLD</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SGLD(Optimizer):
    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._mean: list[tf.Tensor] = []
        self._sum = 0
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                noise = tfp.distributions.Normal(
                        tf.zeros(grad.shape),
                        tf.ones(grad.shape) / sqrt(self._lr(self._n))
                    ).sample()
                var.assign_add(-self._lr(self._n) * (grad + noise)) 

        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = (mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

                # update the second moment
                sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                self._sq_mean[bayesian_layer_index] = sq_mean

                # update the deviation matrix
                deviation_matrix = self._dev[bayesian_layer_index]
                if deviation_matrix.shape[0] == self._hyperparameters.k:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
                else:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix, theta - mean), axis=1)
                bayesian_layer_index += 1
        self._n += 1
        return loss
        
        
    def _init_arrays(self):
        &#34;&#34;&#34;
        initialise arrays to keep track of mean, sq_mean, standard deviation
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                size += tf.size(w).numpy()
            if size != 0:
                self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)

    def update_parameters_step(self):
        return super().update_parameters_step()
        
    def compile_extra_components(self, **kwargs):
        self._batch_size = self._hyperparameters.batch_size
        self._lr = self._hyperparameters.lr
        self._base_model = tf.keras.models.model_from_json(self._model_config)
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._init_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0

    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            tf.debugging.check_numerics(dev, &#34;dev&#34;)
            tf.debugging.check_numerics(mean, &#34;mean&#34;)
            tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)

            tf_dist = tfp.distributions.Normal(
                tf.reshape(mean, (-1,)),
                tf.reshape(sq_mean - mean ** 2, (-1,))
            )

            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.SGLD.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._batch_size = self._hyperparameters.batch_size
    self._lr = self._hyperparameters.lr
    self._base_model = tf.keras.models.model_from_json(self._model_config)
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._init_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SGLD.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        tf.debugging.check_numerics(dev, &#34;dev&#34;)
        tf.debugging.check_numerics(mean, &#34;mean&#34;)
        tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)

        tf_dist = tfp.distributions.Normal(
            tf.reshape(mean, (-1,)),
            tf.reshape(sq_mean - mean ** 2, (-1,))
        )

        tf_dist = TensorflowProbabilityDistribution(
            tf_dist
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SGLD.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            noise = tfp.distributions.Normal(
                    tf.zeros(grad.shape),
                    tf.ones(grad.shape) / sqrt(self._lr(self._n))
                ).sample()
            var.assign_add(-self._lr(self._n) * (grad + noise)) 

    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            
            mean = self._mean[bayesian_layer_index]
            sq_mean = self._sq_mean[bayesian_layer_index]

            # update the mean
            mean = (mean * self._n + theta) / (self._n + 1.0)
            self._mean[bayesian_layer_index] = mean

            # update the second moment
            sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
            self._sq_mean[bayesian_layer_index] = sq_mean

            # update the deviation matrix
            deviation_matrix = self._dev[bayesian_layer_index]
            if deviation_matrix.shape[0] == self._hyperparameters.k:
                self._dev[bayesian_layer_index] = tf.concat(
                    (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
            else:
                self._dev[bayesian_layer_index] = tf.concat(
                    (deviation_matrix, theta - mean), axis=1)
            bayesian_layer_index += 1
    self._n += 1
    return loss</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SGLD.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    return super().update_parameters_step()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.SWAG"><code class="flex name class">
<span>class <span class="ident">SWAG</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SWAG(Optimizer):

    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._frequency = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                var.assign_sub(self._lr * grad)  # assign_sub for SGD update

        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                if self._n % self._hyperparameters.frequency == 0:
                    mean = self._mean[bayesian_layer_index]
                    sq_mean = self._sq_mean[bayesian_layer_index]

                    # update the mean
                    mean = (mean * self._n + theta) / (self._n + 1.0)
                    self._mean[bayesian_layer_index] = mean

                    # update the second moment
                    sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                    self._sq_mean[bayesian_layer_index] = sq_mean

                    # update the deviation matrix
                    deviation_matrix = self._dev[bayesian_layer_index]
                    if deviation_matrix.shape[0] == self._hyperparameters.k:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
                    else:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix, theta - mean), axis=1)
                bayesian_layer_index += 1
        self._n += 1
        return loss


    def compile_extra_components(self, **kwargs):
        self._k = self._hyperparameters.k
        self._frequency = self._hyperparameters.frequency
        self._lr = self._hyperparameters.lr
        self._scale = self._hyperparameters.scale
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(1))
        self._init_swag_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0

    def _init_swag_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                size += tf.size(w).numpy()
            if size != 0:
                self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)

    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            tf.debugging.check_numerics(dev, &#34;dev&#34;)
            tf.debugging.check_numerics(mean, &#34;mean&#34;)
            tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
            #TODO add scale
            tf_dist = MultivariateNormalDiagPlusLowRank(
                tf.reshape(mean, (-1,)),
                tf.reshape(sq_mean - mean ** 2, (-1,)),
                sqrt((1 / (self._k - 1))) * dev,
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.SWAG.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._k = self._hyperparameters.k
    self._frequency = self._hyperparameters.frequency
    self._lr = self._hyperparameters.lr
    self._scale = self._hyperparameters.scale
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(1))
    self._init_swag_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        tf.debugging.check_numerics(dev, &#34;dev&#34;)
        tf.debugging.check_numerics(mean, &#34;mean&#34;)
        tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
        #TODO add scale
        tf_dist = MultivariateNormalDiagPlusLowRank(
            tf.reshape(mean, (-1,)),
            tf.reshape(sq_mean - mean ** 2, (-1,)),
            sqrt((1 / (self._k - 1))) * dev,
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            var.assign_sub(self._lr * grad)  # assign_sub for SGD update

    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            if self._n % self._hyperparameters.frequency == 0:
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = (mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

                # update the second moment
                sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                self._sq_mean[bayesian_layer_index] = sq_mean

                # update the deviation matrix
                deviation_matrix = self._dev[bayesian_layer_index]
                if deviation_matrix.shape[0] == self._hyperparameters.k:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
                else:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix, theta - mean), axis=1)
            bayesian_layer_index += 1
    self._n += 1
    return loss</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="PyAce.optimizers.SWAG"><code class="flex name class">
<span>class <span class="ident">tempCodeRunnerFile</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SWAG(Optimizer):

    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._frequency = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                var.assign_sub(self._lr * grad)  # assign_sub for SGD update

        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                if self._n % self._hyperparameters.frequency == 0:
                    mean = self._mean[bayesian_layer_index]
                    sq_mean = self._sq_mean[bayesian_layer_index]

                    # update the mean
                    mean = (mean * self._n + theta) / (self._n + 1.0)
                    self._mean[bayesian_layer_index] = mean

                    # update the second moment
                    sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                    self._sq_mean[bayesian_layer_index] = sq_mean

                    # update the deviation matrix
                    deviation_matrix = self._dev[bayesian_layer_index]
                    if deviation_matrix.shape[0] == self._hyperparameters.k:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
                    else:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix, theta - mean), axis=1)
                bayesian_layer_index += 1
        self._n += 1
        return loss


    def compile_extra_components(self, **kwargs):
        self._k = self._hyperparameters.k
        self._frequency = self._hyperparameters.frequency
        self._lr = self._hyperparameters.lr
        self._scale = self._hyperparameters.scale
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(1))
        self._init_swag_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0

    def _init_swag_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                size += tf.size(w).numpy()
            if size != 0:
                self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)

    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            tf.debugging.check_numerics(dev, &#34;dev&#34;)
            tf.debugging.check_numerics(mean, &#34;mean&#34;)
            tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
            #TODO add scale
            tf_dist = MultivariateNormalDiagPlusLowRank(
                tf.reshape(mean, (-1,)),
                tf.reshape(sq_mean - mean ** 2, (-1,)),
                sqrt((1 / (self._k - 1))) * dev,
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>PyAce.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="PyAce.optimizers.SWAG.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>used to compile components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    self._k = self._hyperparameters.k
    self._frequency = self._hyperparameters.frequency
    self._lr = self._hyperparameters.lr
    self._scale = self._hyperparameters.scale
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(1))
    self._init_swag_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> PyAce.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        tf.debugging.check_numerics(dev, &#34;dev&#34;)
        tf.debugging.check_numerics(mean, &#34;mean&#34;)
        tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
        #TODO add scale
        tf_dist = MultivariateNormalDiagPlusLowRank(
            tf.reshape(mean, (-1,)),
            tf.reshape(sq_mean - mean ** 2, (-1,)),
            sqrt((1 / (self._k - 1))) * dev,
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>TODO : Add loss return signature
Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>_type_</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            var.assign_sub(self._lr * grad)  # assign_sub for SGD update

    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            if self._n % self._hyperparameters.frequency == 0:
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = (mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

                # update the second moment
                sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                self._sq_mean[bayesian_layer_index] = sq_mean

                # update the deviation matrix
                deviation_matrix = self._dev[bayesian_layer_index]
                if deviation_matrix.shape[0] == self._hyperparameters.k:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix[:, :self._hyperparameters.k - 1], theta - mean), axis=1)
                else:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix, theta - mean), axis=1)
            bayesian_layer_index += 1
    self._n += 1
    return loss</code></pre>
</details>
</dd>
<dt id="PyAce.optimizers.SWAG.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="PyAce" href="../index.html">PyAce</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="PyAce.optimizers.hyperparameters" href="hyperparameters/index.html">PyAce.optimizers.hyperparameters</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="PyAce.optimizers.BBB" href="#PyAce.optimizers.BBB">BBB</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.BBB.compile_extra_components" href="#PyAce.optimizers.BBB.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.BBB.result" href="#PyAce.optimizers.BBB.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.BBB.step" href="#PyAce.optimizers.BBB.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.BBB.update_parameters_step" href="#PyAce.optimizers.BBB.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.FSVI" href="#PyAce.optimizers.FSVI">FSVI</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.FSVI.compile_extra_components" href="#PyAce.optimizers.FSVI.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.predict_region" href="#PyAce.optimizers.FSVI.predict_region">predict_region</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.sample_predicts" href="#PyAce.optimizers.FSVI.sample_predicts">sample_predicts</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.sample_training" href="#PyAce.optimizers.FSVI.sample_training">sample_training</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.step" href="#PyAce.optimizers.FSVI.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.vi_fwd_noise" href="#PyAce.optimizers.FSVI.vi_fwd_noise">vi_fwd_noise</a></code></li>
<li><code><a title="PyAce.optimizers.FSVI.vi_noise" href="#PyAce.optimizers.FSVI.vi_noise">vi_noise</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.HMC" href="#PyAce.optimizers.HMC">HMC</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.HMC.compile_extra_components" href="#PyAce.optimizers.HMC.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.HMC.result" href="#PyAce.optimizers.HMC.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.HMC.step" href="#PyAce.optimizers.HMC.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.HMC.train" href="#PyAce.optimizers.HMC.train">train</a></code></li>
<li><code><a title="PyAce.optimizers.HMC.update_parameters_step" href="#PyAce.optimizers.HMC.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.Optimizer" href="#PyAce.optimizers.Optimizer">Optimizer</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.Optimizer.compile" href="#PyAce.optimizers.Optimizer.compile">compile</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.compile_extra_components" href="#PyAce.optimizers.Optimizer.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.result" href="#PyAce.optimizers.Optimizer.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.step" href="#PyAce.optimizers.Optimizer.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.train" href="#PyAce.optimizers.Optimizer.train">train</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.train_with_weights_and_biases" href="#PyAce.optimizers.Optimizer.train_with_weights_and_biases">train_with_weights_and_biases</a></code></li>
<li><code><a title="PyAce.optimizers.Optimizer.update_parameters_step" href="#PyAce.optimizers.Optimizer.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.SGLD" href="#PyAce.optimizers.SGLD">SGLD</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.SGLD.compile_extra_components" href="#PyAce.optimizers.SGLD.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.SGLD.result" href="#PyAce.optimizers.SGLD.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.SGLD.step" href="#PyAce.optimizers.SGLD.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.SGLD.update_parameters_step" href="#PyAce.optimizers.SGLD.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.SWAG" href="#PyAce.optimizers.SWAG">SWAG</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.SWAG.compile_extra_components" href="#PyAce.optimizers.SWAG.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.result" href="#PyAce.optimizers.SWAG.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.step" href="#PyAce.optimizers.SWAG.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.update_parameters_step" href="#PyAce.optimizers.SWAG.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="PyAce.optimizers.SWAG" href="#PyAce.optimizers.SWAG">SWAG</a></code></h4>
<ul class="">
<li><code><a title="PyAce.optimizers.SWAG.compile_extra_components" href="#PyAce.optimizers.SWAG.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.result" href="#PyAce.optimizers.SWAG.result">result</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.step" href="#PyAce.optimizers.SWAG.step">step</a></code></li>
<li><code><a title="PyAce.optimizers.SWAG.update_parameters_step" href="#PyAce.optimizers.SWAG.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>