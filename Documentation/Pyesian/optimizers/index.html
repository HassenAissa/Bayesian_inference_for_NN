<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Pyesian.optimizers API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Pyesian.optimizers</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .Optimizer import Optimizer
from .BBB import BBB
from .HMC import HMC
from .SGLD import SGLD
from .SWAG import SWAG
from .ADAM import ADAM
from .BSAM import BSAM
from .SGD import SGD
from .VADAM import VADAM

from os.path import dirname, basename, isfile, join
import glob
modules = glob.glob(join(dirname(__file__), &#34;*.py&#34;))
__all__ = [ basename(f)[:-3] for f in modules if isfile(f) and not f.endswith(&#39;__init__.py&#39;)]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="Pyesian.optimizers.hyperparameters" href="hyperparameters/index.html">Pyesian.optimizers.hyperparameters</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Pyesian.optimizers.ADAM"><code class="flex name class">
<span>class <span class="ident">ADAM</span></span>
</code></dt>
<dd>
<div class="desc"><p>ADAM is a class that inherits from Optimizer. </p>
<p>This inference methods is taken from the paper : "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam". </p>
<p><a href="https://arxiv.org/pdf/1806.04854.pdf">https://arxiv.org/pdf/1806.04854.pdf</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr</code>: the learning rate </p>
<p><code>beta_1</code>: average weight between the old first moment value and its gradient. Should be between 0 and 1. </p>
<p><code>beta_2</code>: average weight between the old second moment value and its gradient. Should be between 0 and 1.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ADAM(Optimizer):
    &#34;&#34;&#34;
    ADAM is a class that inherits from Optimizer. \n
    This inference methods is taken from the paper : &#34;Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam&#34;. \n
    https://arxiv.org/pdf/1806.04854.pdf \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr`: the learning rate \n
        `beta_1`: average weight between the old first moment value and its gradient. Should be between 0 and 1. \n
        `beta_2`: average weight between the old second moment value and its gradient. Should be between 0 and 1. \n
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._frequency = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._running_loss = 0
        self._seen_batches = 0
        self._total_batches = 0
        self._epoch_num = 1

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        self._seen_batches += 1
        self._total_batches += 1
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
            self._data_iterator = iter(self._dataloader)
            self._seen_batches = 1
            self._running_loss = 0
            self._epoch_num += 1
            sample, label = next(self._data_iterator, (None, None))
        
        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss(reduction= &#39;none&#39;)(label, predictions)
            self._running_loss += tf.reduce_mean(loss)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            layer = self._base_model.layers[layer_idx]
            for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
                    var_grad = tape.jacobian(loss, sublayer)
                    # print(var_grad.shape, &#34;c&#39;est moi&#34;)

                    var_grad_squared = var_grad ** 2

                    var_grad_squared = tf.reduce_mean(var_grad_squared, axis = 0)
                    var_grad = tf.reduce_mean(var_grad, axis = 0)

                    # print(var_grad)
                    self._m[layer_idx][sublayer_idx] = (self._beta_1 * self._m[layer_idx][sublayer_idx]
                                                       + (1 - self._beta_1) * var_grad)
                    self._v[layer_idx][sublayer_idx] = (self._beta_2 * self._v[layer_idx][sublayer_idx]
                                                       + (1 - self._beta_2) * var_grad_squared)
                    self._m_hat[layer_idx][sublayer_idx] = (self._m[layer_idx][sublayer_idx] /
                                                          (1 - (self._beta_1**self._epoch_num)))
                    self._v_hat[layer_idx][sublayer_idx] = (self._v[layer_idx][sublayer_idx] /
                                                          (1 - (self._beta_2**self._epoch_num)))
                    sublayer.assign_sub(self._lr * self._m_hat[layer_idx][sublayer_idx]/
                                   (tf.sqrt(self._v_hat[layer_idx][sublayer_idx]) + 1e-3)) 

        
        return self._running_loss / self._seen_batches

    def _init_adam_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        self._m = []
        self._m_hat = []
        self._v = []
        self._v_hat = []
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            if len(layer.trainable_variables) != 0:
                m_list = []
                m_hat_list = []
                v_list = []
                v_hat_list = []
                for var in layer.trainable_variables:
                    m_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    m_hat_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    v_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    v_hat_list.append(tf.zeros(var.shape, dtype=tf.float32))
                self._m.append(m_list)
                self._m_hat.append(m_hat_list)
                self._v.append(v_list)
                self._v_hat.append(v_hat_list)
            else:
                self._m.append(None)
                self._m_hat.append(None)
                self._v.append(None)
                self._v_hat.append(None)     

    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                starting_model: this is the starting model for the inference method. It could be a pretrained model.
        &#34;&#34;&#34;
        self._lr = self._hyperparameters.lr
        self._batch_size = self._hyperparameters.batch_size
        self._beta_1 = self._hyperparameters.beta_1
        self._beta_2 = self._hyperparameters.beta_2
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._init_adam_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0


    def result(self) -&gt; BayesianModel:

        model = BayesianModel(self._model_config)
        
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            if len(layer.trainable_variables) != 0:
                mean = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                mean = tf.reshape(tf.concat(mean, 0), (-1,))
                
                tf_dist = tfp.distributions.Deterministic(mean)
                tf_dist = TensorflowProbabilityDistribution(
                    tf_dist
                )
                model.apply_distribution(tf_dist, layer_idx, layer_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.ADAM.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>starting_model</code></strong></dt>
<dd>this is the starting model for the inference method. It could be a pretrained model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            starting_model: this is the starting model for the inference method. It could be a pretrained model.
    &#34;&#34;&#34;
    self._lr = self._hyperparameters.lr
    self._batch_size = self._hyperparameters.batch_size
    self._beta_1 = self._hyperparameters.beta_1
    self._beta_2 = self._hyperparameters.beta_2
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._init_adam_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.ADAM.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:

    model = BayesianModel(self._model_config)
    
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        if len(layer.trainable_variables) != 0:
            mean = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            mean = tf.reshape(tf.concat(mean, 0), (-1,))
            
            tf_dist = tfp.distributions.Deterministic(mean)
            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )
            model.apply_distribution(tf_dist, layer_idx, layer_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.ADAM.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    self._seen_batches += 1
    self._total_batches += 1
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
        self._data_iterator = iter(self._dataloader)
        self._seen_batches = 1
        self._running_loss = 0
        self._epoch_num += 1
        sample, label = next(self._data_iterator, (None, None))
    
    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss(reduction= &#39;none&#39;)(label, predictions)
        self._running_loss += tf.reduce_mean(loss)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        layer = self._base_model.layers[layer_idx]
        for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
                var_grad = tape.jacobian(loss, sublayer)
                # print(var_grad.shape, &#34;c&#39;est moi&#34;)

                var_grad_squared = var_grad ** 2

                var_grad_squared = tf.reduce_mean(var_grad_squared, axis = 0)
                var_grad = tf.reduce_mean(var_grad, axis = 0)

                # print(var_grad)
                self._m[layer_idx][sublayer_idx] = (self._beta_1 * self._m[layer_idx][sublayer_idx]
                                                   + (1 - self._beta_1) * var_grad)
                self._v[layer_idx][sublayer_idx] = (self._beta_2 * self._v[layer_idx][sublayer_idx]
                                                   + (1 - self._beta_2) * var_grad_squared)
                self._m_hat[layer_idx][sublayer_idx] = (self._m[layer_idx][sublayer_idx] /
                                                      (1 - (self._beta_1**self._epoch_num)))
                self._v_hat[layer_idx][sublayer_idx] = (self._v[layer_idx][sublayer_idx] /
                                                      (1 - (self._beta_2**self._epoch_num)))
                sublayer.assign_sub(self._lr * self._m_hat[layer_idx][sublayer_idx]/
                               (tf.sqrt(self._v_hat[layer_idx][sublayer_idx]) + 1e-3)) 

    
    return self._running_loss / self._seen_batches</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.ADAM.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.BBB"><code class="flex name class">
<span>class <span class="ident">BBB</span></span>
</code></dt>
<dd>
<div class="desc"><p>BBB is a class that inherits from Optimizer.</p>
<p>This inference method is taken from the paper : "Weight Uncertainty in Neural Networks". </p>
<p><a href="https://arxiv.org/pdf/1505.05424.pdf.">https://arxiv.org/pdf/1505.05424.pdf.</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128 </p>
<p><code>lr</code>: the learning rate </p>
<p><code>pi</code>: A weight to average between the first and the second prior (only if we have a single prior for the network).
If we have a single prior this hyperparameter is ignored.
This value should be between 0 and 1. </p>
<p><code>alpha</code>: the scale of the KL divergence in the loss function. It should be between 0 and 1</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BBB(Optimizer):

    &#34;&#34;&#34;
    BBB is a class that inherits from Optimizer.\n
    This inference method is taken from the paper : &#34;Weight Uncertainty in Neural Networks&#34;. \n
    https://arxiv.org/pdf/1505.05424.pdf. \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128 \n
        `lr`: the learning rate \n
        `pi`: A weight to average between the first and the second prior (only if we have a single prior for the network).
            If we have a single prior this hyperparameter is ignored. 
            This value should be between 0 and 1. \n
        `alpha`: the scale of the KL divergence in the loss function. It should be between 0 and 1 \n
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()
        self._data_iterator = None
        self._dataloader = None
        self._lr = None
        self._alpha = None
        self._base_model: tf.keras.Model = None
        self._priors_list = None
        self._weight_layers_indices = []
        self._posterior_mean_list = []
        self._posterior_std_dev_list = []
        self._priors_list = []
        self._layers_intervals = []
        self._prior2: GaussianPrior = None
        self._prior1: GaussianPrior = None
        self._prior: GaussianPrior = None
        self._step = 0



    def _guassian_likelihood(self,weights, mean, std_dev):
        &#34;&#34;&#34;
        calculates the likelihood of the weights being from a guassian distribution of the given mean and standard deviation

        Args:
            weights (tf.Tensor): the weights to test
            mean (tf.Tensor): the mean
            std_dev (tf.Tensor): the standard deviation

        Returns:
            tf.Tensor: the likelihood
        &#34;&#34;&#34;
        guassian_distribution = tfp.distributions.Normal(mean, tf.math.softplus(std_dev))
        return tf.reduce_sum(guassian_distribution.log_prob(weights))

    def _prior_guassian_likelihood(self):
        &#34;&#34;&#34;
        calculates the guassian likelihood of the weights with respect to the prior
        Returns:
            tf.Tensor: the likelihood of the weights of being sampled from the prior
        &#34;&#34;&#34;
        likelihood = 0
        mean_idx = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            for i in range(len(layer.trainable_variables)):
                likelihood += self._guassian_likelihood(
                    layer.trainable_variables[i], 
                    self._priors_list[mean_idx][i].mean(),
                    self._priors_list[mean_idx][i].stddev(),
                )
            mean_idx += 1

        return likelihood
    
    def _posterior_guassian_likelihood(self):
        &#34;&#34;&#34;
        calculates the guassian likelihood of the weights with respect to the posterior
        Returns:
            tf.Tensor: the likelihood of the weights of being sampled from the posterior
        &#34;&#34;&#34;
        likelihood = 0
        mean_idx = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            for i in range(len(layer.trainable_variables)):
                likelihood += self._guassian_likelihood(
                    layer.trainable_variables[i], 
                    self._posterior_mean_list[mean_idx][i], 
                    self._posterior_std_dev_list[mean_idx][i]
                )
            if len(layer.trainable_variables) != 0:
                mean_idx += 1

        return likelihood

    def _cost_function(self, labels: tf.Tensor, predictions: tf.Tensor):
        &#34;&#34;&#34;
        calculate the cost function as follows:
        cost = data_likelihood + alpha * kl_divergence

        Args:
            labels (tf.Tensor): the labels
            predictions (tf.Tensor): the predictions

        Returns:
            tf.Tensor: the cost
        &#34;&#34;&#34;
        posterior_likelihood = self._posterior_guassian_likelihood()
        prior_likelihood = self._prior_guassian_likelihood()
        kl_divergence = tf.math.subtract(posterior_likelihood, prior_likelihood)
        data_likelihood = self._dataset.loss()(labels, predictions)
        kl_divergence = tf.multiply(kl_divergence, self._alpha)
        return tf.math.add(data_likelihood, kl_divergence)
    


    def step(self, save_document_path = None):
        self._step += 1
        #update the weights
        noises = self._update_weights()

        # get sample and label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            # take the posterior distribution into account in the calculation of the gradients
            tape.watch(self._posterior_mean_list)
            tape.watch(self._posterior_std_dev_list)
            predictions = self._base_model(sample, training = True)
            likelihood = self._cost_function(
                label, 
                predictions
            )
        # get the weight, mean and standard deviation gradients
        mean_gradients = tape.gradient(likelihood, self._posterior_mean_list)
        std_dev_gradients = tape.gradient(likelihood, self._posterior_std_dev_list)

        # save the model loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(likelihood.numpy())+&#34;\n&#34;)

        new_posterior_mean_list = []
        new_posterior_std_dev_list = []
        trainable_layer_index = 0
        gradient_layer_index = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            if len(layer.trainable_variables) != 0:
                # go through weights and biases and merge their gradients into one vector
                _new_mean_layer_weights = []
                _new_std_dev_layer_weights = []

                for i in range(len(layer.trainable_variables)):
                    # calculate the new mean of the posterior
                    weight_gradients = tape.gradient(likelihood, layer.trainable_variables[i])

                    mean_gradient = mean_gradients[trainable_layer_index][i]+weight_gradients
                    _new_mean_layer_weights.append(
                        self._posterior_mean_list[trainable_layer_index][i]-self._lr*mean_gradient
                    )
                    # calculate the std_dev gradient
                    posterior_std_dev = self._posterior_std_dev_list[trainable_layer_index][i]
                    noise = noises[gradient_layer_index]
                    std_dev_grad = noise / (1 + tf.math.exp(-posterior_std_dev))
                    std_dev_grad *= weight_gradients
                    std_dev_grad += std_dev_gradients[trainable_layer_index][i]
                    # calculate the new standard deviation of the posterior
                    _new_std_dev_layer_weights.append(
                        posterior_std_dev-self._lr*std_dev_grad
                    )
                    gradient_layer_index += 1
                trainable_layer_index += 1
                new_posterior_mean_list.append(_new_mean_layer_weights)
                new_posterior_std_dev_list.append(_new_std_dev_layer_weights)

        #update the posteriors
        self._posterior_mean_list = new_posterior_mean_list
        self._posterior_std_dev_list = new_posterior_std_dev_list
        return likelihood






    def _update_weights(self):
        &#34;&#34;&#34;
        generate new weights following the posterior distributions
        &#34;&#34;&#34;
        noises = []
        for interval_idx in range(len(self._layers_intervals)):
            # reshape the vector and update the base model
            start = self._layers_intervals[interval_idx][0]
            end = self._layers_intervals[interval_idx][1]
            for layer_idx in range(start, end + 1):
                # go through weights and biases of the layer
                for i in range(len(self._base_model.layers[layer_idx].trainable_variables)):
                    #sample the new wights as a vector
                    w = self._base_model.layers[layer_idx].trainable_variables[i]
                    noise = tfp.distributions.Normal(
                        tf.zeros(self._posterior_mean_list[interval_idx][i].shape),
                        tf.ones(self._posterior_std_dev_list[interval_idx][i].shape)
                    ).sample()
                    noises.append(noise)
                    vector_weights = noise * tf.math.softplus(self._posterior_std_dev_list[interval_idx][i])
                    vector_weights += self._posterior_mean_list[interval_idx][i]

                    new_weights = tf.reshape(vector_weights, w.shape)
                    self._base_model.layers[layer_idx].trainable_variables[i].assign(new_weights)
        return noises



    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            Args:
                prior (GaussianPrior): the prior
                prior2 (GuassianPrior): second prior used to create a mixture prior weighted with hyperparameter pi
        &#34;&#34;&#34;
        self._base_model = tf.keras.models.model_from_json(self._model_config)
        self._prior = kwargs[&#34;prior&#34;]
        if &#34;prior2&#34; in kwargs:
            self._prior2 = kwargs[&#34;prior2&#34;]
        else:  
            self._prior2 = GaussianPrior(0.0,0.0)
        self._lr = self._hyperparameters.lr
        self._pi = getattr(self._hyperparameters, &#39;pi&#39;, None) if hasattr(self._hyperparameters, &#39;pi&#39;) else 1
        self._batch_size = int(self._hyperparameters.batch_size)
        if isinstance(self._prior._mean, int) or isinstance(self._prior._mean, float):
            sign = self._prior._std_dev/abs(self._prior._std_dev)
            self._prior = GaussianPrior(
                self._prior._mean * self._pi + self._prior2._mean * (1 - self._pi),
                sign * math.sqrt((self._prior._std_dev * self._pi)**2 + (self._prior2._std_dev * (1 - self._pi))**2),
            )

        self._alpha = self._hyperparameters.alpha
        self._dataset_setup()
        self._priors_list = self._prior.get_model_priors(self._base_model)
        self._init_BBB_arrays()

    def _init_BBB_arrays(self):
        &#34;&#34;&#34;
        initialises the posterior list, the correlated layers intervals and the trainable layer indices
        &#34;&#34;&#34;
        trainable_layer_index = 0
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            # iterate through weights and biases of the layer
            mean_layer_posteriors = []
            std_dev_layer_posteriors = []

            if len(layer.trainable_variables) != 0:
                for i in range(len(layer.trainable_variables)):
                    mean_layer_posteriors.append(self._priors_list[trainable_layer_index][i].mean())
                    std_dev_layer_posteriors.append(self._priors_list[trainable_layer_index][i].stddev())
                self._weight_layers_indices.append(layer_idx)
                self._layers_intervals.append([layer_idx, layer_idx])
                self._posterior_mean_list.append(mean_layer_posteriors)
                self._posterior_std_dev_list.append(std_dev_layer_posteriors)
            trainable_layer_index += 1



    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for layer_mean_list, layer_std_dev_list, idx in zip(self._posterior_mean_list, self._posterior_std_dev_list, range(len(self._weight_layers_indices))):
            for i in range(len(layer_mean_list)):
                layer_mean_list[i] = tf.reshape(layer_mean_list[i], (-1,))
                layer_std_dev_list[i]= tf.reshape(layer_std_dev_list[i], (-1,))
            mean = tf.concat(layer_mean_list, 0)
            std_dev = tf.concat(layer_std_dev_list,0)
            # tf.debugging.check_numerics(mean, &#34;mean&#34;)
            # tf.debugging.check_numerics(std_dev, &#34;standard deviation&#34;)
            tf_dist = tfp.distributions.Normal(
                tf.reshape(mean, (-1,)),
                tf.math.softplus(tf.reshape(std_dev, (-1,)))
            )
            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )

            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]
            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.BBB.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>GaussianPrior</code></dt>
<dd>the prior</dd>
<dt><strong><code>prior2</code></strong> :&ensp;<code>GuassianPrior</code></dt>
<dd>second prior used to create a mixture prior weighted with hyperparameter pi</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        Args:
            prior (GaussianPrior): the prior
            prior2 (GuassianPrior): second prior used to create a mixture prior weighted with hyperparameter pi
    &#34;&#34;&#34;
    self._base_model = tf.keras.models.model_from_json(self._model_config)
    self._prior = kwargs[&#34;prior&#34;]
    if &#34;prior2&#34; in kwargs:
        self._prior2 = kwargs[&#34;prior2&#34;]
    else:  
        self._prior2 = GaussianPrior(0.0,0.0)
    self._lr = self._hyperparameters.lr
    self._pi = getattr(self._hyperparameters, &#39;pi&#39;, None) if hasattr(self._hyperparameters, &#39;pi&#39;) else 1
    self._batch_size = int(self._hyperparameters.batch_size)
    if isinstance(self._prior._mean, int) or isinstance(self._prior._mean, float):
        sign = self._prior._std_dev/abs(self._prior._std_dev)
        self._prior = GaussianPrior(
            self._prior._mean * self._pi + self._prior2._mean * (1 - self._pi),
            sign * math.sqrt((self._prior._std_dev * self._pi)**2 + (self._prior2._std_dev * (1 - self._pi))**2),
        )

    self._alpha = self._hyperparameters.alpha
    self._dataset_setup()
    self._priors_list = self._prior.get_model_priors(self._base_model)
    self._init_BBB_arrays()</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BBB.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for layer_mean_list, layer_std_dev_list, idx in zip(self._posterior_mean_list, self._posterior_std_dev_list, range(len(self._weight_layers_indices))):
        for i in range(len(layer_mean_list)):
            layer_mean_list[i] = tf.reshape(layer_mean_list[i], (-1,))
            layer_std_dev_list[i]= tf.reshape(layer_std_dev_list[i], (-1,))
        mean = tf.concat(layer_mean_list, 0)
        std_dev = tf.concat(layer_std_dev_list,0)
        # tf.debugging.check_numerics(mean, &#34;mean&#34;)
        # tf.debugging.check_numerics(std_dev, &#34;standard deviation&#34;)
        tf_dist = tfp.distributions.Normal(
            tf.reshape(mean, (-1,)),
            tf.math.softplus(tf.reshape(std_dev, (-1,)))
        )
        tf_dist = TensorflowProbabilityDistribution(
            tf_dist
        )

        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]
        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BBB.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    self._step += 1
    #update the weights
    noises = self._update_weights()

    # get sample and label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        # take the posterior distribution into account in the calculation of the gradients
        tape.watch(self._posterior_mean_list)
        tape.watch(self._posterior_std_dev_list)
        predictions = self._base_model(sample, training = True)
        likelihood = self._cost_function(
            label, 
            predictions
        )
    # get the weight, mean and standard deviation gradients
    mean_gradients = tape.gradient(likelihood, self._posterior_mean_list)
    std_dev_gradients = tape.gradient(likelihood, self._posterior_std_dev_list)

    # save the model loss if the path is specified
    if save_document_path != None:
        with open(save_document_path, &#34;a&#34;) as losses_file:
            losses_file.write(str(likelihood.numpy())+&#34;\n&#34;)

    new_posterior_mean_list = []
    new_posterior_std_dev_list = []
    trainable_layer_index = 0
    gradient_layer_index = 0
    for layer_idx in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_idx]
        if len(layer.trainable_variables) != 0:
            # go through weights and biases and merge their gradients into one vector
            _new_mean_layer_weights = []
            _new_std_dev_layer_weights = []

            for i in range(len(layer.trainable_variables)):
                # calculate the new mean of the posterior
                weight_gradients = tape.gradient(likelihood, layer.trainable_variables[i])

                mean_gradient = mean_gradients[trainable_layer_index][i]+weight_gradients
                _new_mean_layer_weights.append(
                    self._posterior_mean_list[trainable_layer_index][i]-self._lr*mean_gradient
                )
                # calculate the std_dev gradient
                posterior_std_dev = self._posterior_std_dev_list[trainable_layer_index][i]
                noise = noises[gradient_layer_index]
                std_dev_grad = noise / (1 + tf.math.exp(-posterior_std_dev))
                std_dev_grad *= weight_gradients
                std_dev_grad += std_dev_gradients[trainable_layer_index][i]
                # calculate the new standard deviation of the posterior
                _new_std_dev_layer_weights.append(
                    posterior_std_dev-self._lr*std_dev_grad
                )
                gradient_layer_index += 1
            trainable_layer_index += 1
            new_posterior_mean_list.append(_new_mean_layer_weights)
            new_posterior_std_dev_list.append(_new_std_dev_layer_weights)

    #update the posteriors
    self._posterior_mean_list = new_posterior_mean_list
    self._posterior_std_dev_list = new_posterior_std_dev_list
    return likelihood</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BBB.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.BSAM"><code class="flex name class">
<span>class <span class="ident">BSAM</span></span>
</code></dt>
<dd>
<div class="desc"><p>ADAM is a class that inherits from Optimizer. </p>
<p>This inference methods is taken from the paper : "SAM AS AN OPTIMAL RELAXATION OF BAYES". </p>
<p><a href="https://arxiv.org/pdf/2210.01620.pdf">https://arxiv.org/pdf/2210.01620.pdf</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr</code>: the learning rate </p>
<p><code>beta_1</code>: average weight between the old first moment value and its gradient. Should be between 0 and 1. </p>
<p><code>beta_2</code>: average weight between the old second moment value and its gradient. Should be between 0 and 1. </p>
<p><code>lam</code>: precision parameter </p>
<p><code>rho</code>: sharpness aware parameter. Set to 0.01 or less </p>
<p><code>gam</code>: . Should be set to 1e-1</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BSAM(Optimizer):
    &#34;&#34;&#34;
    ADAM is a class that inherits from Optimizer. \n
    This inference methods is taken from the paper : &#34;SAM AS AN OPTIMAL RELAXATION OF BAYES&#34;. \n
    https://arxiv.org/pdf/2210.01620.pdf \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr`: the learning rate \n
        `beta_1`: average weight between the old first moment value and its gradient. Should be between 0 and 1. \n
        `beta_2`: average weight between the old second moment value and its gradient. Should be between 0 and 1. \n
        `lam`: precision parameter \n
        `rho`: sharpness aware parameter. Set to 0.01 or less \n
        `gam`: . Should be set to 1e-1 \n
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._running_loss = 0
        self._seen_batches = 0
        self._total_batches = 0
        self._epoch_num = 1
        self._lam = 0.5

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        self._seen_batches += 1
        self._total_batches += 1
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
            self._data_iterator = iter(self._dataloader)
            self._seen_batches = 1
            self._running_loss = 0
            self._epoch_num += 1
            sample, label = next(self._data_iterator, (None, None))
        predictions = self._base_model(sample)


        # Start by adding noise to the parameter 
        for layer, layer_ind in zip(self._base_model.layers, range(len(self._base_model.layers))):
            for var, var_ind in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
                eps = tf.random.normal(shape=var.shape, mean = 0.0, stddev=1.0)
                sigma = 1/(self._num_data * (self._v[layer_ind][var_ind]))
                #print(&#34;peturb norm: &#34;, tf.norm(sigma))
                var.assign_add(eps * sigma)

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            loss = self._dataset.loss()(label, predictions)
            self._running_loss += loss
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        # Updating with the addition of rho
        layer_grads = []
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            var_grad = tape.gradient(loss, layer.trainable_variables)
            it_val = 0
            orig_grads = []
            for var, grad, sub_layer_idx in zip(layer.trainable_variables, var_grad, range(len(layer.trainable_variables))):
                if grad is not None:
                    orig_grads.append(grad)
                    e = self._rho * (grad/self._v[layer_idx][sub_layer_idx])
                    #print(&#34;SHAPES: &#34;)
                    #print(self._v[it_val].shape, grad.shape, var.shape, e.shape)
                    var.assign_add(e)
            layer_grads.append(orig_grads)
         
        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            loss = self._dataset.loss()(label, predictions)
            self._running_loss += loss
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        for layer, layer_ind in zip(self._base_model.layers, range(len(self._base_model.layers))):
            var_grad = tape.gradient(loss, layer.trainable_variables)
            for var, grad, sub_layer_idx in zip(layer.trainable_variables, var_grad, range(len(layer.trainable_variables))):
                if grad is not None:
                    #print(&#34;NORMS: &#34;)
                    #print(tf.norm(grad))

                    self._m[layer_ind][sub_layer_idx] = self._beta_1 * self._m[layer_ind][sub_layer_idx]
                    self._m[layer_ind][sub_layer_idx] += (1 - self._beta_1) * (grad + (self._lam * var))

                    self._v[layer_ind][sub_layer_idx] = self._beta_2 * self._v[layer_ind][sub_layer_idx]
                    self._v[layer_ind][sub_layer_idx] += (1 - self._beta_2) * (tf.sqrt(self._v[layer_ind][sub_layer_idx])
                                                                               * tf.abs(layer_grads[layer_ind][sub_layer_idx] + self._lam + self._gam))

                    var.assign_sub(self._lr * self._m[layer_ind][sub_layer_idx]/self._v[layer_ind][sub_layer_idx])

        return self._running_loss / self._seen_batches

    def _init_bsam_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        self._m = []
        self._v = []
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            if len(layer.trainable_variables) != 0:
                m_list = []
                m_hat_list = []
                v_list = []
                for var in layer.trainable_variables:
                    m_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    m_hat_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    v_list.append(tf.ones(var.shape, dtype=tf.float32))
                self._m.append(m_list)
                self._v.append(v_list)
            else:
                self._m.append(None)
                self._v.append(None)

                
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                starting_model: this is the starting model for the inference method. It could be a pretrained model.
        &#34;&#34;&#34;
        self._lr = self._hyperparameters.lr
        self._beta_1 = self._hyperparameters.beta_1
        self._beta_2 = self._hyperparameters.beta_2
        self._batch_size = self._hyperparameters.batch_size
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._init_bsam_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0
        self._lam = self._hyperparameters.lam
        self._rho = self._hyperparameters.rho
        self._gam = self._hyperparameters.gam
        self._num_data = self._dataset.training_dataset().cardinality().numpy().item()
        
    def result(self) -&gt; BayesianModel:
        # for x in self._v:
        #     print(x.shape)
        model = BayesianModel(self._model_config)

        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            if len(layer.trainable_variables) != 0:
                mean = [tf.dtypes.cast(tf.reshape(w, (-1, 1)), dtype = tf.float32) for w in layer.trainable_variables]
                mean = tf.reshape(tf.concat(mean, 0), (-1, 1))
                var = [tf.cast(tf.reshape(1/(self._num_data*v), (-1, 1)), dtype=tf.float32) for v in self._v[layer_idx]]
                var = tf.reshape(tf.concat(var, 0), (-1, 1))

                tf_dist = tfp.distributions.Normal(loc=tf.reshape(mean, (-1,)), scale=tf.reshape(var, (-1,)))
                tf_dist = TensorflowProbabilityDistribution(tf_dist)
                model.apply_distribution(tf_dist, layer_idx, layer_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.BSAM.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>starting_model</code></strong></dt>
<dd>this is the starting model for the inference method. It could be a pretrained model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            starting_model: this is the starting model for the inference method. It could be a pretrained model.
    &#34;&#34;&#34;
    self._lr = self._hyperparameters.lr
    self._beta_1 = self._hyperparameters.beta_1
    self._beta_2 = self._hyperparameters.beta_2
    self._batch_size = self._hyperparameters.batch_size
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._init_bsam_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0
    self._lam = self._hyperparameters.lam
    self._rho = self._hyperparameters.rho
    self._gam = self._hyperparameters.gam
    self._num_data = self._dataset.training_dataset().cardinality().numpy().item()</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BSAM.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    # for x in self._v:
    #     print(x.shape)
    model = BayesianModel(self._model_config)

    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        if len(layer.trainable_variables) != 0:
            mean = [tf.dtypes.cast(tf.reshape(w, (-1, 1)), dtype = tf.float32) for w in layer.trainable_variables]
            mean = tf.reshape(tf.concat(mean, 0), (-1, 1))
            var = [tf.cast(tf.reshape(1/(self._num_data*v), (-1, 1)), dtype=tf.float32) for v in self._v[layer_idx]]
            var = tf.reshape(tf.concat(var, 0), (-1, 1))

            tf_dist = tfp.distributions.Normal(loc=tf.reshape(mean, (-1,)), scale=tf.reshape(var, (-1,)))
            tf_dist = TensorflowProbabilityDistribution(tf_dist)
            model.apply_distribution(tf_dist, layer_idx, layer_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BSAM.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    self._seen_batches += 1
    self._total_batches += 1
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
        self._data_iterator = iter(self._dataloader)
        self._seen_batches = 1
        self._running_loss = 0
        self._epoch_num += 1
        sample, label = next(self._data_iterator, (None, None))
    predictions = self._base_model(sample)


    # Start by adding noise to the parameter 
    for layer, layer_ind in zip(self._base_model.layers, range(len(self._base_model.layers))):
        for var, var_ind in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
            eps = tf.random.normal(shape=var.shape, mean = 0.0, stddev=1.0)
            sigma = 1/(self._num_data * (self._v[layer_ind][var_ind]))
            #print(&#34;peturb norm: &#34;, tf.norm(sigma))
            var.assign_add(eps * sigma)

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        loss = self._dataset.loss()(label, predictions)
        self._running_loss += loss
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    # Updating with the addition of rho
    layer_grads = []
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        var_grad = tape.gradient(loss, layer.trainable_variables)
        it_val = 0
        orig_grads = []
        for var, grad, sub_layer_idx in zip(layer.trainable_variables, var_grad, range(len(layer.trainable_variables))):
            if grad is not None:
                orig_grads.append(grad)
                e = self._rho * (grad/self._v[layer_idx][sub_layer_idx])
                #print(&#34;SHAPES: &#34;)
                #print(self._v[it_val].shape, grad.shape, var.shape, e.shape)
                var.assign_add(e)
        layer_grads.append(orig_grads)
     
    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        loss = self._dataset.loss()(label, predictions)
        self._running_loss += loss
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    for layer, layer_ind in zip(self._base_model.layers, range(len(self._base_model.layers))):
        var_grad = tape.gradient(loss, layer.trainable_variables)
        for var, grad, sub_layer_idx in zip(layer.trainable_variables, var_grad, range(len(layer.trainable_variables))):
            if grad is not None:
                #print(&#34;NORMS: &#34;)
                #print(tf.norm(grad))

                self._m[layer_ind][sub_layer_idx] = self._beta_1 * self._m[layer_ind][sub_layer_idx]
                self._m[layer_ind][sub_layer_idx] += (1 - self._beta_1) * (grad + (self._lam * var))

                self._v[layer_ind][sub_layer_idx] = self._beta_2 * self._v[layer_ind][sub_layer_idx]
                self._v[layer_ind][sub_layer_idx] += (1 - self._beta_2) * (tf.sqrt(self._v[layer_ind][sub_layer_idx])
                                                                           * tf.abs(layer_grads[layer_ind][sub_layer_idx] + self._lam + self._gam))

                var.assign_sub(self._lr * self._m[layer_ind][sub_layer_idx]/self._v[layer_ind][sub_layer_idx])

    return self._running_loss / self._seen_batches</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.BSAM.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.HMC"><code class="flex name class">
<span>class <span class="ident">HMC</span></span>
</code></dt>
<dd>
<div class="desc"><p>HMC is an class that inherits from Optimizer. </p>
<p>This powerful inference method is taken from: "MCMC Using Hamiltonian Dynamics". </p>
<p><a href="https://www.mcmchandbook.net/HandbookChapter5.pdf">https://www.mcmchandbook.net/HandbookChapter5.pdf</a>
</p>
<p>It takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>m</code>: m is the mass of the puck </p>
<p><code>L</code>: Number of leapfrog updates to do in one run </p>
<p><code>epsilon</code>: step size for leapfrog step</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HMC(Optimizer):
    &#34;&#34;&#34;
    HMC is an class that inherits from Optimizer. \n
    This powerful inference method is taken from: &#34;MCMC Using Hamiltonian Dynamics&#34;. \n
    https://www.mcmchandbook.net/HandbookChapter5.pdf  \n
    It takes the following hyperparameters:
    Hyperparameters:
        `m`: m is the mass of the puck \n
        `L`: Number of leapfrog updates to do in one run \n
        `epsilon`: step size for leapfrog step \n
        
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._nb_burn_epoch = 10
        self._X = None
        self._y = None
        self._training_dataset_cardinality = None
        self._burn_epochs = None
        self._training_dataset = None
        self._prior = None
        self._p = None
        self._frequency = None
        self._samples = None
        self._model = None
        self._epsilon = None
        self._L = None
        self._m = None
        self._total_runs = 0
        self._accepted_runs = 0
        self._current_loss = 0

    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                prior (GaussianPrior): the model priors
                nb_burn_epochs (int): number of burn epochs

        &#34;&#34;&#34;
        self._m = self._hyperparameters.m
        self._L = self._hyperparameters.L
        self._epsilon = self._hyperparameters.epsilon
        self._model: tf.keras.models.Model = tf.keras.models.model_from_json(self._model_config)
        self._samples = []
        self._frequency = []
        self._p = []
        self._prior = kwargs[&#34;prior&#34;].get_model_priors(self._model)
        if &#34;nb_burn_epoch&#34; in kwargs:
            self._nb_burn_epoch = kwargs[&#34;nb_burn_epochs&#34;]
        self._batch_size = self._dataset.training_dataset().cardinality()
        self._dataset_setup()
        self._X, self._y = next(iter(self._training_dataset.batch(self._batch_size)))
        for layer in self._model.layers:
            self._p.append([tf.Variable(tf.zeros(w.shape)) for w in layer.trainable_variables])

        for layer, distribs in zip(self._model.layers, self._prior):
            if len(layer.trainable_variables) &gt; 0:
                for w, d in zip(layer.trainable_variables, distribs):
                    w.assign(d.mean())

    def step(self, save_document_path=None, sampling=True, burning=False):
        if len(self._frequency) == 0 and sampling:
            self._frequency.append(1)
            self._samples.append(self._snapshot_q())
        self._sample_kinetic_energy()
        current_k = self._kinetic_energy()
        current_u, current_loss = self._potential_energy()
        current_q = self._snapshot_q()
        self._step_p(self._epsilon / 2)
        for i in range(self._L):
            self._step_q(self._epsilon)
            if i != self._L:
                self._step_p(self._epsilon)
        self._step_p(self._epsilon / 2)
        new_k = self._kinetic_energy()
        new_u, new_loss = self._potential_energy()
        self._total_runs += 1
        if burning or random.random() &lt; tf.math.exp(current_k + current_u - new_k - new_u).numpy().item():
            self._accepted_runs += 1
            if sampling:
                self._frequency.append(1)
                self._samples.append(self._snapshot_q())
            return new_loss
        else:
            # if rejected restore old layers variables
            for layer, current_layer in zip(self._model.layers, current_q):
                for w, q in zip(layer.trainable_variables, current_layer):
                    w.assign(q)
            if sampling:
                self._frequency[len(self._frequency) - 1] += 1
            return current_loss

    def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
              model_save_path: str = None):
        self._accepted_runs = 0
        self._total_runs = 0
        nb_burn_epoch = self._nb_burn_epoch
        for i in range(nb_burn_epoch):
            loss = self.step(sampling=False, burning=True)
            accept_rate = self._accepted_runs / self._total_runs
            self._print_progress((i + 1) / nb_burn_epoch, suffix=&#34;HMC - Burning&#34;, loss=loss.numpy().item(),
                                 accept_rate=accept_rate, bar_length=20)
        self._new_progress_line()
        self._accepted_runs = 0
        self._total_runs = 0
        self._frequency = []
        self._samples = []
        for i in range(nb_iterations):
            loss = self.step(sampling=True, burning=False)
            accept_rate = self._accepted_runs / self._total_runs
            self._print_progress((i + 1) / nb_iterations, suffix=&#34;HMC - Sampling&#34;, loss=loss.numpy().item(),
                                 accept_rate=accept_rate, bar_length=20)
        self._new_progress_line()

    def _step_p(self, step_size):
        with tf.GradientTape(persistent=True) as tape:
            tape.watch(self._model.trainable_variables)
            U, _ = self._potential_energy()
        for layer, momentum_layer in zip(self._model.layers, self._p):
            for q, p in zip(layer.trainable_variables, momentum_layer):
                q_grad = tape.gradient(U, q)
                p.assign_sub(tf.multiply(q_grad, step_size))
        del tape  # free the gradient resources

    def _step_q(self, step_size):
        for layer, momentum_layer in zip(self._model.layers, self._p):
            for q, p in zip(layer.trainable_variables, momentum_layer):
                q.assign_add(tf.multiply(p, step_size / self._m))

    def _snapshot_q(self):
        q = []
        for layer in self._model.layers:
            q.append([tf.identity(w) for w in layer.trainable_variables])
        return q

    def _potential_energy(self) -&gt; (tf.Tensor, tf.Tensor):
        potential_energy = tf.constant([0.0])
        for layer, distribs in zip(self._model.layers, self._prior):
            if len(layer.trainable_variables) &gt; 0:
                for w, d in zip(layer.trainable_variables, distribs):
                    potential_energy -= tf.math.reduce_sum(d.log_prob(w))
        predictions = self._model(self._X)
        # the loss is already the log likelihood of the data in this case
        loss = self._dataset.loss()(self._y, predictions)
        potential_energy += loss * self._training_dataset_cardinality
        return potential_energy, loss

    def _kinetic_energy(self):
        kinetic_energy = tf.constant([0.0])
        for layer in self._p:
            for var_p in layer:
                kinetic_energy += (1 / (2 * self._m)) * tf.math.reduce_sum(tf.square(var_p))
        return kinetic_energy

    def _sample_kinetic_energy(self):
        for layer in self._p:
            for w in layer:
                w.assign(tf.random.normal(w.shape, stddev=self._m, mean=0))

    def update_parameters_step(self):
        pass

    def result(self) -&gt; BayesianModel:
        samples_unrolled = []
        for sample in self._samples:
            concat_unrolled = []
            for sample_layer in sample:
                for w in sample_layer:
                    concat_unrolled.append(tf.reshape(w, (-1,)))
            samples_unrolled.append(tf.concat(concat_unrolled, axis=0))
        distribution = Sampled(samples_unrolled, self._frequency)
        posterior_model = BayesianModel(self._model_config)
        posterior_model.apply_distribution(distribution, 0, len(self._model.layers) - 1)
        return posterior_model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.HMC.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prior</code></strong> :&ensp;<code>GaussianPrior</code></dt>
<dd>the model priors</dd>
<dt><strong><code>nb_burn_epochs</code></strong> :&ensp;<code>int</code></dt>
<dd>number of burn epochs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            prior (GaussianPrior): the model priors
            nb_burn_epochs (int): number of burn epochs

    &#34;&#34;&#34;
    self._m = self._hyperparameters.m
    self._L = self._hyperparameters.L
    self._epsilon = self._hyperparameters.epsilon
    self._model: tf.keras.models.Model = tf.keras.models.model_from_json(self._model_config)
    self._samples = []
    self._frequency = []
    self._p = []
    self._prior = kwargs[&#34;prior&#34;].get_model_priors(self._model)
    if &#34;nb_burn_epoch&#34; in kwargs:
        self._nb_burn_epoch = kwargs[&#34;nb_burn_epochs&#34;]
    self._batch_size = self._dataset.training_dataset().cardinality()
    self._dataset_setup()
    self._X, self._y = next(iter(self._training_dataset.batch(self._batch_size)))
    for layer in self._model.layers:
        self._p.append([tf.Variable(tf.zeros(w.shape)) for w in layer.trainable_variables])

    for layer, distribs in zip(self._model.layers, self._prior):
        if len(layer.trainable_variables) &gt; 0:
            for w, d in zip(layer.trainable_variables, distribs):
                w.assign(d.mean())</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.HMC.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    samples_unrolled = []
    for sample in self._samples:
        concat_unrolled = []
        for sample_layer in sample:
            for w in sample_layer:
                concat_unrolled.append(tf.reshape(w, (-1,)))
        samples_unrolled.append(tf.concat(concat_unrolled, axis=0))
    distribution = Sampled(samples_unrolled, self._frequency)
    posterior_model = BayesianModel(self._model_config)
    posterior_model.apply_distribution(distribution, 0, len(self._model.layers) - 1)
    return posterior_model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.HMC.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None, sampling=True, burning=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path=None, sampling=True, burning=False):
    if len(self._frequency) == 0 and sampling:
        self._frequency.append(1)
        self._samples.append(self._snapshot_q())
    self._sample_kinetic_energy()
    current_k = self._kinetic_energy()
    current_u, current_loss = self._potential_energy()
    current_q = self._snapshot_q()
    self._step_p(self._epsilon / 2)
    for i in range(self._L):
        self._step_q(self._epsilon)
        if i != self._L:
            self._step_p(self._epsilon)
    self._step_p(self._epsilon / 2)
    new_k = self._kinetic_energy()
    new_u, new_loss = self._potential_energy()
    self._total_runs += 1
    if burning or random.random() &lt; tf.math.exp(current_k + current_u - new_k - new_u).numpy().item():
        self._accepted_runs += 1
        if sampling:
            self._frequency.append(1)
            self._samples.append(self._snapshot_q())
        return new_loss
    else:
        # if rejected restore old layers variables
        for layer, current_layer in zip(self._model.layers, current_q):
            for w, q in zip(layer.trainable_variables, current_layer):
                w.assign(q)
        if sampling:
            self._frequency[len(self._frequency) - 1] += 1
        return current_loss</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.HMC.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None, model_save_path: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>trains the model and saved the training metrics and model status</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nb_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>number of training iterations</dd>
<dt><strong><code>loss_save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the loss during training. Defaults to None.</dd>
<dt><strong><code>model_save_frequency</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The frequency of saving the models during training. Defaults to None.</dd>
<dt><strong><code>model_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the models during training. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>if the model saving path is specified and the frequency of saving the model is not, or</dd>
</dl>
<p>if the frequency of saving the model is sprecified and the model saving path is not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
          model_save_path: str = None):
    self._accepted_runs = 0
    self._total_runs = 0
    nb_burn_epoch = self._nb_burn_epoch
    for i in range(nb_burn_epoch):
        loss = self.step(sampling=False, burning=True)
        accept_rate = self._accepted_runs / self._total_runs
        self._print_progress((i + 1) / nb_burn_epoch, suffix=&#34;HMC - Burning&#34;, loss=loss.numpy().item(),
                             accept_rate=accept_rate, bar_length=20)
    self._new_progress_line()
    self._accepted_runs = 0
    self._total_runs = 0
    self._frequency = []
    self._samples = []
    for i in range(nb_iterations):
        loss = self.step(sampling=True, burning=False)
        accept_rate = self._accepted_runs / self._total_runs
        self._print_progress((i + 1) / nb_iterations, suffix=&#34;HMC - Sampling&#34;, loss=loss.numpy().item(),
                             accept_rate=accept_rate, bar_length=20)
    self._new_progress_line()</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.HMC.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.Optimizer"><code class="flex name class">
<span>class <span class="ident">Optimizer</span></span>
</code></dt>
<dd>
<div class="desc"><p>Helper class that provides a standard way to create an ABC using
inheritance.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Optimizer(ABC):

    def __init__(self):
        self._model_config = None
        self._hyperparameters = None
        self.__compiled = False
        self._dataset: Dataset = None

    @abstractmethod
    def step(self, save_document_path=None):
        &#34;&#34;&#34;
        Performs one step of the training

        Args:
            save_document_path (str, optional): The path to save the losses during the training. Defaults to None.

        Returns:
            float: the loss value after the step
        &#34;&#34;&#34;
        pass

    def _dataset_setup(self):
        self._training_dataset: tf.data.Dataset = self._dataset.training_dataset()
        self._training_dataset_cardinality = self._training_dataset.cardinality().numpy().item()
        self._dataloader = (self._training_dataset
                            .shuffle(self._training_dataset_cardinality)
                            .batch(self._batch_size))
        self._data_iterator = iter(self._dataloader)

    def compile(self, hyperparameters: HyperParameters, model_config: str, dataset, verbose=True, **kwargs):
        &#34;&#34;&#34;compile the model

        Args:
            hyperparameters (HyperParameters): the model hyperparameters
            model_config (dict): the configuration of the model
            dataset (Dataset): the dataset of the model

        Raises:
            Exception: raises error if the model is already compiled
        &#34;&#34;&#34;
        if self.__compiled:
            raise Exception(&#34;Model Already compiled&#34;)
        else:
            self.__compiled = True
            self._hyperparameters = hyperparameters
            self._model_config = model_config
            self._dataset = dataset
            self._verbose = verbose
        self.compile_extra_components(**kwargs)

    @abstractmethod
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
        &#34;&#34;&#34;
        pass

    @abstractmethod
    def update_parameters_step(self):
        &#34;&#34;&#34;
        one step of updating the model parameters
        &#34;&#34;&#34;
        pass

    def _empty_folder(self, path):
        for filename in os.listdir(path):
            file_path = os.path.join(path, filename)
            try:
                if os.path.isfile(file_path) or os.path.islink(file_path):
                    os.unlink(file_path)
                elif os.path.isdir(file_path):
                    shutil.rmtree(file_path)
            except Exception as e:
                print(&#39;Failed to delete %s. Reason: %s&#39; % (file_path, e))

    def train_with_weights_and_biases(self, nb_iterations, project_name, weights_and_biases_config):
        wandb.login()
        run = wandb.init(project=project_name, config=weights_and_biases_config)
        self.train(nb_iterations, weights_and_biases_log=True)

    def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
              model_save_path: str = None, weights_and_biases_log=False):
        &#34;&#34;&#34;
        trains the model and saved the training metrics and model status

        Args:
            nb_iterations (int): number of training iterations
            loss_save_document_path (str, optional): The path to save the loss during training. Defaults to None.
            model_save_frequency (int, optional): The frequency of saving the models during training. Defaults to None.
            model_save_path (str, optional): The path to save the models during training. Defaults to None.

        Raises:
            Exception: if the model saving path is specified and the frequency of saving the model is not, or
            if the frequency of saving the model is sprecified and the model saving path is not.
        &#34;&#34;&#34;
        if model_save_frequency == None and model_save_path != None:
            raise Exception(&#34;Error: save path precised and save frequency is None, please provide a savong frequency&#34;)
        if model_save_frequency != None and model_save_path == None:
            raise Exception(&#34;Error: save frequency precised and save path is None, please provide a saving path&#34;)

        if loss_save_document_path != None and os.path.exists(loss_save_document_path):
            os.remove(loss_save_document_path)

        if model_save_path != None:
            self._empty_folder(model_save_path)

        saved_model_nbr = 0
        for i in range(nb_iterations):
            loss = self.step(loss_save_document_path)
            self._print_progress(i / nb_iterations, loss=loss)
            if weights_and_biases_log == True:
                wandb.log({
                    &#34;loss&#34;: loss
                })
            if model_save_frequency != None and i % model_save_frequency == 0:
                bayesian_model = self.result()
                if os.path.exists(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr))):
                    shutil.rmtree(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                os.makedirs(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                bayesian_model.store(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
                saved_model_nbr += 1

        if self._verbose:
            print()

    @abstractmethod
    def result(self) -&gt; BayesianModel:
        &#34;&#34;&#34;
        create a bayesian model at the stage of the training

        Returns:
            BayesianModel: the bayesian model trained
        &#34;&#34;&#34;
        pass

    def _print_progress(self, progress: float, bar_length=10, suffix=&#34;Training&#34;, **kwargs):
        if not self._verbose:
            return
        nb_chars = math.ceil(progress * bar_length)
        bar = &#34;[&#34; + nb_chars * &#34;=&#34;
        if nb_chars &lt; bar_length:
            bar += &#34;&gt;&#34;
        bar += &#34;]&#34;
        infos = &#39; &#39;.join(&#34;{}: {}&#34;.format(k, v) for k, v in kwargs.items())
        percentage = str(math.ceil(progress * 100))
        print(&#34;\r&#34; + suffix + &#34; &#34; + percentage + &#34; % &#34; + bar + &#34; &#34; + infos, end=&#34;&#34;)

    def _new_progress_line(self):
        if not self._verbose:
            return
        print()</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li>Pyesian.optimizers.ADAM.ADAM</li>
<li>Pyesian.optimizers.BBB.BBB</li>
<li>Pyesian.optimizers.BSAM.BSAM</li>
<li>Pyesian.optimizers.HMC.HMC</li>
<li>Pyesian.optimizers.SGD.SGD</li>
<li>Pyesian.optimizers.SGLD.SGLD</li>
<li>Pyesian.optimizers.SWAG.SWAG</li>
<li>Pyesian.optimizers.VADAM.VADAM</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.Optimizer.compile"><code class="name flex">
<span>def <span class="ident">compile</span></span>(<span>self, hyperparameters: <a title="Pyesian.optimizers.hyperparameters.HyperParameters.HyperParameters" href="hyperparameters/HyperParameters.html#Pyesian.optimizers.hyperparameters.HyperParameters.HyperParameters">HyperParameters</a>, model_config: str, dataset, verbose=True, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compile the model</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>hyperparameters</code></strong> :&ensp;<code>HyperParameters</code></dt>
<dd>the model hyperparameters</dd>
<dt><strong><code>model_config</code></strong> :&ensp;<code>dict</code></dt>
<dd>the configuration of the model</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>the dataset of the model</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>raises error if the model is already compiled</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile(self, hyperparameters: HyperParameters, model_config: str, dataset, verbose=True, **kwargs):
    &#34;&#34;&#34;compile the model

    Args:
        hyperparameters (HyperParameters): the model hyperparameters
        model_config (dict): the configuration of the model
        dataset (Dataset): the dataset of the model

    Raises:
        Exception: raises error if the model is already compiled
    &#34;&#34;&#34;
    if self.__compiled:
        raise Exception(&#34;Model Already compiled&#34;)
    else:
        self.__compiled = True
        self._hyperparameters = hyperparameters
        self._model_config = model_config
        self._dataset = dataset
        self._verbose = verbose
    self.compile_extra_components(**kwargs)</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def result(self) -&gt; BayesianModel:
    &#34;&#34;&#34;
    create a bayesian model at the stage of the training

    Returns:
        BayesianModel: the bayesian model trained
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def step(self, save_document_path=None):
    &#34;&#34;&#34;
    Performs one step of the training

    Args:
        save_document_path (str, optional): The path to save the losses during the training. Defaults to None.

    Returns:
        float: the loss value after the step
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None, model_save_path: str = None, weights_and_biases_log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>trains the model and saved the training metrics and model status</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nb_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>number of training iterations</dd>
<dt><strong><code>loss_save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the loss during training. Defaults to None.</dd>
<dt><strong><code>model_save_frequency</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The frequency of saving the models during training. Defaults to None.</dd>
<dt><strong><code>model_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the models during training. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>if the model saving path is specified and the frequency of saving the model is not, or</dd>
</dl>
<p>if the frequency of saving the model is sprecified and the model saving path is not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
          model_save_path: str = None, weights_and_biases_log=False):
    &#34;&#34;&#34;
    trains the model and saved the training metrics and model status

    Args:
        nb_iterations (int): number of training iterations
        loss_save_document_path (str, optional): The path to save the loss during training. Defaults to None.
        model_save_frequency (int, optional): The frequency of saving the models during training. Defaults to None.
        model_save_path (str, optional): The path to save the models during training. Defaults to None.

    Raises:
        Exception: if the model saving path is specified and the frequency of saving the model is not, or
        if the frequency of saving the model is sprecified and the model saving path is not.
    &#34;&#34;&#34;
    if model_save_frequency == None and model_save_path != None:
        raise Exception(&#34;Error: save path precised and save frequency is None, please provide a savong frequency&#34;)
    if model_save_frequency != None and model_save_path == None:
        raise Exception(&#34;Error: save frequency precised and save path is None, please provide a saving path&#34;)

    if loss_save_document_path != None and os.path.exists(loss_save_document_path):
        os.remove(loss_save_document_path)

    if model_save_path != None:
        self._empty_folder(model_save_path)

    saved_model_nbr = 0
    for i in range(nb_iterations):
        loss = self.step(loss_save_document_path)
        self._print_progress(i / nb_iterations, loss=loss)
        if weights_and_biases_log == True:
            wandb.log({
                &#34;loss&#34;: loss
            })
        if model_save_frequency != None and i % model_save_frequency == 0:
            bayesian_model = self.result()
            if os.path.exists(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr))):
                shutil.rmtree(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            os.makedirs(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            bayesian_model.store(os.path.join(model_save_path, &#34;model&#34; + str(saved_model_nbr)))
            saved_model_nbr += 1

    if self._verbose:
        print()</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.train_with_weights_and_biases"><code class="name flex">
<span>def <span class="ident">train_with_weights_and_biases</span></span>(<span>self, nb_iterations, project_name, weights_and_biases_config)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_with_weights_and_biases(self, nb_iterations, project_name, weights_and_biases_config):
    wandb.login()
    run = wandb.init(project=project_name, config=weights_and_biases_config)
    self.train(nb_iterations, weights_and_biases_log=True)</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.Optimizer.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abstractmethod
def update_parameters_step(self):
    &#34;&#34;&#34;
    one step of updating the model parameters
    &#34;&#34;&#34;
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.SGD"><code class="flex name class">
<span>class <span class="ident">SGD</span></span>
</code></dt>
<dd>
<div class="desc"><p>SGD is a class that inherits from Optimizer. </p>
<p>This simple inference methods is taken from the paper : "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble". </p>
<p><a href="https://arxiv.org/pdf/1612.01474.pdf">https://arxiv.org/pdf/1612.01474.pdf</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr</code>: the learning rate </p>
<p><code>frequency</code>: moment update frequency. It could be left to 1 and increased for performance reasons.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SGD(Optimizer):
    &#34;&#34;&#34;
    SGD is a class that inherits from Optimizer. \n
    This simple inference methods is taken from the paper : &#34;Simple and Scalable Predictive Uncertainty Estimation using Deep Ensemble&#34;. \n
    https://arxiv.org/pdf/1612.01474.pdf \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr`: the learning rate \n
        `frequency`: moment update frequency. It could be left to 1 and increased for performance reasons. \n
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._frequency = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []
        self._running_loss = 0
        self._seen_batches = 0
        self._epoch_num = 1

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        self._seen_batches += 1
        
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
            self._data_iterator = iter(self._dataloader)
            self._seen_batches = 1
            self._running_loss = 0
            self._epoch_num += 1
            sample, label = next(self._data_iterator, (None, None))
        
        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            self._running_loss += loss
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))
        
        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                var.assign_sub(self._lr * grad)  # assign_sub for SGD update
        
        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                if self._n % self._frequency == 0:
                    mean = self._mean[bayesian_layer_index]
                    sq_mean = self._sq_mean[bayesian_layer_index]

                    # update the mean
                    mean = theta #(mean * self._n + theta) / (self._n + 1.0)
                    self._mean[bayesian_layer_index] = mean

                bayesian_layer_index += 1
        self._n += 1 
        
        return self._running_loss / self._seen_batches

    def _init_sgd_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                if(size == 0):
                    init_val = tf.dtypes.cast(tf.reshape(w, (-1)), dtype=tf.float32)
                else: 
                    init_val = tf.concat((init_val, tf.dtypes.cast(tf.reshape(w, (-1)), dtype=tf.float32)), axis=0)
                size += tf.size(w).numpy()
            # print(init_val)
            if size != 0:
                #self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._mean.append(tf.expand_dims(init_val, axis=-1))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)
                
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                starting_model: this is the starting model for the inference method. It could be a pretrained model.
        &#34;&#34;&#34;
        self._frequency = self._hyperparameters.frequency
        self._lr = self._hyperparameters.lr
        self._batch_size = self._hyperparameters.batch_size
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._init_sgd_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0


    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            # tf.debugging.check_numerics(mean, &#34;mean&#34;)
            tf_dist =tfp.distributions.Deterministic(tf.reshape(mean, (-1,)))
            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.SGD.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>starting_model</code></strong></dt>
<dd>this is the starting model for the inference method. It could be a pretrained model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            starting_model: this is the starting model for the inference method. It could be a pretrained model.
    &#34;&#34;&#34;
    self._frequency = self._hyperparameters.frequency
    self._lr = self._hyperparameters.lr
    self._batch_size = self._hyperparameters.batch_size
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._init_sgd_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGD.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        # tf.debugging.check_numerics(mean, &#34;mean&#34;)
        tf_dist =tfp.distributions.Deterministic(tf.reshape(mean, (-1,)))
        tf_dist = TensorflowProbabilityDistribution(
            tf_dist
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGD.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    self._seen_batches += 1
    
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
        self._data_iterator = iter(self._dataloader)
        self._seen_batches = 1
        self._running_loss = 0
        self._epoch_num += 1
        sample, label = next(self._data_iterator, (None, None))
    
    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        self._running_loss += loss
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))
    
    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            var.assign_sub(self._lr * grad)  # assign_sub for SGD update
    
    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            if self._n % self._frequency == 0:
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = theta #(mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

            bayesian_layer_index += 1
    self._n += 1 
    
    return self._running_loss / self._seen_batches</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGD.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.SGLD"><code class="flex name class">
<span>class <span class="ident">SGLD</span></span>
</code></dt>
<dd>
<div class="desc"><p>SGLD is a class that inherits from Optimizer. </p>
<p>This inference methods is taken from the paper : "Bayesian Learning via Stochastic Gradient Langevin Dynamics". </p>
<p><a href="https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf">https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr_upper</code>: the learning rate at step 0 </p>
<p><code>lr_lower</code>: the learning rate at the last step </p>
<p><code>lr_gamma</code>: controls rate of change of learning rate [0.5, 1.0)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SGLD(Optimizer):
    &#34;&#34;&#34;
    SGLD is a class that inherits from Optimizer. \n
    This inference methods is taken from the paper : &#34;Bayesian Learning via Stochastic Gradient Langevin Dynamics&#34;. \n
    https://www.stats.ox.ac.uk/~teh/research/compstats/WelTeh2011a.pdf \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr_upper`: the learning rate at step 0 \n
        `lr_lower`: the learning rate at the last step \n
        `lr_gamma`: controls rate of change of learning rate [0.5, 1.0) \n
    &#34;&#34;&#34;


    def __init__(self):
        super().__init__()
        self._n = None
        self._running_loss = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr_upper = None
        self._lr_lower = None
        self._lr_gamma = None
        self._lr = None
        self._mean: list[tf.Tensor] = []
        self._sum = 0
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            self._running_loss += loss
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                noise = tf.random.normal(shape=grad.shape, mean = 0.0, stddev=self._lr(self._n))
                var.assign_add(-self._lr(self._n) * (grad + noise)) 

        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = (mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

                # update the second moment
                sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                self._sq_mean[bayesian_layer_index] = sq_mean

                # update the deviation matrix
                deviation_matrix = self._dev[bayesian_layer_index]
                self._dev[bayesian_layer_index] = tf.concat(
                    (deviation_matrix, theta - mean), axis=1)
                bayesian_layer_index += 1
        self._n += 1
        return self._running_loss / self._n        
        
    def _init_arrays(self):
        &#34;&#34;&#34;
        initialise arrays to keep track of mean, sq_mean, standard deviation
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                size += tf.size(w).numpy()
            if size != 0:
                self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)

    def _init_sgld_lr(self):
        n = self._nb_iterations
        l_g = np.power(self._lr_lower, 1.0 / self._lr_gamma)
        u_g = np.power(self._lr_upper, 1.0 / self._lr_gamma)
        b = -(n * l_g) / (l_g - u_g)
        a = self._lr_upper * np.power(b, self._lr_gamma)
        self._lr = lambda step: a * np.power((b+step), -self._lr_gamma)

    def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
              model_save_path: str = None, weights_and_biases_log = False):
        self._nb_iterations = nb_iterations
        self._init_sgld_lr()
        super().train(nb_iterations, loss_save_document_path, model_save_frequency, model_save_path, weights_and_biases_log)

    def update_parameters_step(self):
        return super().update_parameters_step()
        
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
        &#34;&#34;&#34;
        self._batch_size = int(self._hyperparameters.batch_size)
        self._lr_upper = self._hyperparameters.lr_upper
        self._lr_lower = self._hyperparameters.lr_lower
        self._lr_gamma = self._hyperparameters.lr_gamma
        self._base_model = tf.keras.models.model_from_json(self._model_config)
        self._dataset_setup()
        self._init_arrays()
        self._n = 0
        self._running_loss = 0

    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            # tf.debugging.check_numerics(dev, &#34;dev&#34;)
            # tf.debugging.check_numerics(mean, &#34;mean&#34;)
            # tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)

            tf_dist = tfp.distributions.Normal(
                tf.reshape(mean, (-1,)),
                tf.reshape(sq_mean - mean ** 2, (-1,))
            )

            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.SGLD.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
    &#34;&#34;&#34;
    self._batch_size = int(self._hyperparameters.batch_size)
    self._lr_upper = self._hyperparameters.lr_upper
    self._lr_lower = self._hyperparameters.lr_lower
    self._lr_gamma = self._hyperparameters.lr_gamma
    self._base_model = tf.keras.models.model_from_json(self._model_config)
    self._dataset_setup()
    self._init_arrays()
    self._n = 0
    self._running_loss = 0</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGLD.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        # tf.debugging.check_numerics(dev, &#34;dev&#34;)
        # tf.debugging.check_numerics(mean, &#34;mean&#34;)
        # tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)

        tf_dist = tfp.distributions.Normal(
            tf.reshape(mean, (-1,)),
            tf.reshape(sq_mean - mean ** 2, (-1,))
        )

        tf_dist = TensorflowProbabilityDistribution(
            tf_dist
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGLD.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        self._running_loss += loss
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            noise = tf.random.normal(shape=grad.shape, mean = 0.0, stddev=self._lr(self._n))
            var.assign_add(-self._lr(self._n) * (grad + noise)) 

    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            
            mean = self._mean[bayesian_layer_index]
            sq_mean = self._sq_mean[bayesian_layer_index]

            # update the mean
            mean = (mean * self._n + theta) / (self._n + 1.0)
            self._mean[bayesian_layer_index] = mean

            # update the second moment
            sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
            self._sq_mean[bayesian_layer_index] = sq_mean

            # update the deviation matrix
            deviation_matrix = self._dev[bayesian_layer_index]
            self._dev[bayesian_layer_index] = tf.concat(
                (deviation_matrix, theta - mean), axis=1)
            bayesian_layer_index += 1
    self._n += 1
    return self._running_loss / self._n        </code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGLD.train"><code class="name flex">
<span>def <span class="ident">train</span></span>(<span>self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None, model_save_path: str = None, weights_and_biases_log=False)</span>
</code></dt>
<dd>
<div class="desc"><p>trains the model and saved the training metrics and model status</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>nb_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>number of training iterations</dd>
<dt><strong><code>loss_save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the loss during training. Defaults to None.</dd>
<dt><strong><code>model_save_frequency</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The frequency of saving the models during training. Defaults to None.</dd>
<dt><strong><code>model_save_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the models during training. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>if the model saving path is specified and the frequency of saving the model is not, or</dd>
</dl>
<p>if the frequency of saving the model is sprecified and the model saving path is not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train(self, nb_iterations: int, loss_save_document_path: str = None, model_save_frequency: int = None,
          model_save_path: str = None, weights_and_biases_log = False):
    self._nb_iterations = nb_iterations
    self._init_sgld_lr()
    super().train(nb_iterations, loss_save_document_path, model_save_frequency, model_save_path, weights_and_biases_log)</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SGLD.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    return super().update_parameters_step()</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.SWAG"><code class="flex name class">
<span>class <span class="ident">SWAG</span></span>
</code></dt>
<dd>
<div class="desc"><p>SWAG is a class that inherits from Optimizer. </p>
<p>This simple inference methods is taken from the paper : "A simple baseline for Bayesian uncertianty in deep learning". </p>
<p><a href="https://arxiv.org/pdf/1902.02476.pdf">https://arxiv.org/pdf/1902.02476.pdf</a>
</p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr</code>: the learning rate </p>
<p><code>k</code>: maximum number of columns in the deviation matrix. It should not be very big so that it takes into account
only the last part of the training where we start converging. Should be at least 2. </p>
<p><code>scale</code>: the scale of the deviation matrix. It should be between 0 and 1 </p>
<p><code>frequency</code>: moment update frequency. It could be left to 1 and increased for performance reasons.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SWAG(Optimizer):
    &#34;&#34;&#34;
    SWAG is a class that inherits from Optimizer. \n
    This simple inference methods is taken from the paper : &#34;A simple baseline for Bayesian uncertianty in deep learning&#34;. \n
    https://arxiv.org/pdf/1902.02476.pdf  \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr`: the learning rate \n
        `k`: maximum number of columns in the deviation matrix. It should not be very big so that it takes into account 
        only the last part of the training where we start converging. Should be at least 2. \n
        `scale`: the scale of the deviation matrix. It should be between 0 and 1 \n
        `frequency`: moment update frequency. It could be left to 1 and increased for performance reasons. \n
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._frequency = None
        self._k = None
        self._mean: list[tf.Tensor] = []
        self._sq_mean: list[tf.Tensor] = []
        self._dev: list[tf.Tensor] = []
        self._weight_layers_indices = []

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            self._data_iterator = iter(self._dataloader)
            sample, label = next(self._data_iterator, (None, None))

        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            # get the loss
            loss = self._dataset.loss()(label, predictions)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))

        var_grad = tape.gradient(loss, self._base_model.trainable_variables)
        for var, grad in zip(self._base_model.trainable_variables, var_grad):
            if grad is not None:
                var.assign_sub(self._lr * grad)  # assign_sub for SGD update

        bayesian_layer_index = 0
        for layer_index in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_index]

            if len(layer.trainable_variables) != 0:
                theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
                if self._n % self._frequency == 0:
                    mean = self._mean[bayesian_layer_index]
                    sq_mean = self._sq_mean[bayesian_layer_index]

                    # update the mean
                    mean = (mean * self._n + theta) / (self._n + 1.0)
                    self._mean[bayesian_layer_index] = mean

                    # update the second moment
                    sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                    self._sq_mean[bayesian_layer_index] = sq_mean

                    # update the deviation matrix
                    deviation_matrix = self._dev[bayesian_layer_index]
                    if deviation_matrix.shape[1] == self._k:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix[:, :self._k - 1], theta - mean), axis=1)
                    else:
                        self._dev[bayesian_layer_index] = tf.concat(
                            (deviation_matrix, theta - mean), axis=1)
                bayesian_layer_index += 1
        self._n += 1
        return loss


    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                starting_model: this is the starting model for the inference method. It could be a pretrained model.
        &#34;&#34;&#34;
        self._k = int(self._hyperparameters.k)
        self._frequency = int(self._hyperparameters.frequency)
        self._lr = self._hyperparameters.lr
        self._scale = self._hyperparameters.scale
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._batch_size = int(self._hyperparameters.batch_size)
        self._dataset_setup()
        self._init_swag_arrays()
        self._n = 0

    def _init_swag_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            size = 0
            for w in layer.trainable_variables:
                size += tf.size(w).numpy()
            if size != 0:
                self._mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._sq_mean.append(tf.zeros((size, 1), dtype=tf.float32))
                self._dev.append(tf.zeros((size, 0), dtype=tf.float32))
                self._weight_layers_indices.append(layer_idx)

    def result(self) -&gt; BayesianModel:
        model = BayesianModel(self._model_config)
        for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                           range(len(self._weight_layers_indices))):
            # tf.debugging.check_numerics(dev, &#34;dev&#34;)
            # tf.debugging.check_numerics(mean, &#34;mean&#34;)
            # tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
            tf_dist = MultivariateNormalDiagPlusLowRank(
                tf.reshape(mean, (-1,)),
                tf.reshape(sq_mean - mean ** 2, (-1,)),
                sqrt((self._scale / (self._k - 1))) * dev,
            )
            start_idx = self._weight_layers_indices[idx]
            end_idx = len(self._base_model.layers) - 1
            if idx + 1 &lt; len(self._weight_layers_indices):
                end_idx = self._weight_layers_indices[idx + 1]

            model.apply_distribution(tf_dist, start_idx, start_idx)
        return model

    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.SWAG.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>starting_model</code></strong></dt>
<dd>this is the starting model for the inference method. It could be a pretrained model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            starting_model: this is the starting model for the inference method. It could be a pretrained model.
    &#34;&#34;&#34;
    self._k = int(self._hyperparameters.k)
    self._frequency = int(self._hyperparameters.frequency)
    self._lr = self._hyperparameters.lr
    self._scale = self._hyperparameters.scale
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._batch_size = int(self._hyperparameters.batch_size)
    self._dataset_setup()
    self._init_swag_arrays()
    self._n = 0</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SWAG.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    model = BayesianModel(self._model_config)
    for mean, sq_mean, dev, idx in zip(self._mean, self._sq_mean, self._dev,
                                       range(len(self._weight_layers_indices))):
        # tf.debugging.check_numerics(dev, &#34;dev&#34;)
        # tf.debugging.check_numerics(mean, &#34;mean&#34;)
        # tf.debugging.check_numerics(sq_mean, &#34;sq_meqn&#34;)
        tf_dist = MultivariateNormalDiagPlusLowRank(
            tf.reshape(mean, (-1,)),
            tf.reshape(sq_mean - mean ** 2, (-1,)),
            sqrt((self._scale / (self._k - 1))) * dev,
        )
        start_idx = self._weight_layers_indices[idx]
        end_idx = len(self._base_model.layers) - 1
        if idx + 1 &lt; len(self._weight_layers_indices):
            end_idx = self._weight_layers_indices[idx + 1]

        model.apply_distribution(tf_dist, start_idx, start_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SWAG.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        self._data_iterator = iter(self._dataloader)
        sample, label = next(self._data_iterator, (None, None))

    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        # get the loss
        loss = self._dataset.loss()(label, predictions)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))

    var_grad = tape.gradient(loss, self._base_model.trainable_variables)
    for var, grad in zip(self._base_model.trainable_variables, var_grad):
        if grad is not None:
            var.assign_sub(self._lr * grad)  # assign_sub for SGD update

    bayesian_layer_index = 0
    for layer_index in range(len(self._base_model.layers)):
        layer = self._base_model.layers[layer_index]

        if len(layer.trainable_variables) != 0:
            theta = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            theta = tf.reshape(tf.concat(theta, 0), (-1, 1))
            if self._n % self._frequency == 0:
                mean = self._mean[bayesian_layer_index]
                sq_mean = self._sq_mean[bayesian_layer_index]

                # update the mean
                mean = (mean * self._n + theta) / (self._n + 1.0)
                self._mean[bayesian_layer_index] = mean

                # update the second moment
                sq_mean = (sq_mean * self._n + theta ** 2) / (self._n + 1.0)
                self._sq_mean[bayesian_layer_index] = sq_mean

                # update the deviation matrix
                deviation_matrix = self._dev[bayesian_layer_index]
                if deviation_matrix.shape[1] == self._k:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix[:, :self._k - 1], theta - mean), axis=1)
                else:
                    self._dev[bayesian_layer_index] = tf.concat(
                        (deviation_matrix, theta - mean), axis=1)
            bayesian_layer_index += 1
    self._n += 1
    return loss</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.SWAG.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Pyesian.optimizers.VADAM"><code class="flex name class">
<span>class <span class="ident">VADAM</span></span>
</code></dt>
<dd>
<div class="desc"><p>VADAM is a class that inherits from Optimizer. </p>
<p>This inference methods is taken from the paper : "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam". </p>
<p><a href="https://arxiv.org/pdf/1806.04854.pdf">https://arxiv.org/pdf/1806.04854.pdf</a> </p>
<p>This inference methods takes the following hyperparameters:</p>
<h2 id="hyperparameters">Hyperparameters</h2>
<p><code>batch_size</code>: the size of the batch for one step. Defaults to 128. </p>
<p><code>lr</code>: the learning rate </p>
<p><code>beta_1</code>: average weight between the old first moment value and its gradient. Should be between 0 and 1. </p>
<p><code>beta_2</code>: average weight between the old second moment value and its gradient. Should be between 0 and 1. </p>
<p><code>lam</code>: precision parameter </p>
<p><code>num_data</code>: size of training data</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VADAM(Optimizer):
    &#34;&#34;&#34;

    VADAM is a class that inherits from Optimizer. \n
    This inference methods is taken from the paper : &#34;Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam&#34;. \n
    https://arxiv.org/pdf/1806.04854.pdf \n
    This inference methods takes the following hyperparameters:
    Hyperparameters:
        `batch_size`: the size of the batch for one step. Defaults to 128. \n
        `lr`: the learning rate \n
        `beta_1`: average weight between the old first moment value and its gradient. Should be between 0 and 1. \n
        `beta_2`: average weight between the old second moment value and its gradient. Should be between 0 and 1. \n
        `lam`: precision parameter \n
        `num_data`: size of training data \n
    &#34;&#34;&#34;
    def __init__(self):
        super().__init__()
        self._n = None
        self._data_iterator = None
        self._dataloader = None
        self._base_model_optimizer = None
        self._base_model: tf.keras.Model = None
        self._lr = None
        self._mean: list[tf.Tensor] = []
        self._running_loss = 0
        self._seen_batches = 0
        self._total_batches = 0
        self._epoch_num = 1
        self._lam = 0.5

    def step(self, save_document_path = None):
        # get the sample and the label
        sample,label = next(self._data_iterator, (None,None))
        self._seen_batches += 1
        self._total_batches += 1
        # if the iterator reaches the end of the dataset, reinitialise the iterator
        if sample is None:
            # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
            self._data_iterator = iter(self._dataloader)
            self._seen_batches = 1
            self._running_loss = 0
            self._epoch_num += 1
            sample, label = next(self._data_iterator, (None, None))
        
        # Start by adding noise to the parameter 
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
                eps = tf.random.normal(shape=sublayer.shape, mean = 0.0, stddev=1.0)
                sigma = 1/tf.sqrt(self._num_data * (self._v[layer_idx][sublayer_idx] + self._lam))
                #print(&#34;peturb norm: &#34;, tf.norm(sigma))
                sublayer.assign_add(eps * sigma)
            
        with tf.GradientTape(persistent=True) as tape:
            predictions = self._base_model(sample, training = True)
            loss = self._dataset.loss(reduction= &#39;none&#39;)(label, predictions)
            self._running_loss += tf.reduce_mean(loss)
            # save the loss if the path is specified
            if save_document_path != None:
                with open(save_document_path, &#34;a&#34;) as losses_file:
                    losses_file.write(str(loss.numpy()))
        differentiation_variables = []
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            sublayer_differentiation_variables = []
            for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):   
                sublayer_differentiation_variables.append(sublayer)
            differentiation_variables.append(sublayer_differentiation_variables)
        grad = tape.jacobian(loss, differentiation_variables)
        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            for sublayer, sublayer_idx, sublayer_grad in zip(layer.trainable_variables, range(len(layer.trainable_variables)), grad[layer_idx]):
                 
                sublayer_grad_squared = sublayer_grad ** 2
                sublayer_grad_squared = tf.reduce_mean(sublayer_grad_squared, axis = 0)
                sublayer_grad = tf.reduce_mean(sublayer_grad, axis = 0)
                if sublayer_grad is not None:
                    self._m[layer_idx][sublayer_idx] = self._beta_1 * self._m[layer_idx][sublayer_idx]
                    self._m[layer_idx][sublayer_idx] += (1 - self._beta_1) * (sublayer_grad + (self._lam * sublayer/self._num_data))
                    self._v[layer_idx][sublayer_idx] = self._beta_2 * self._v[layer_idx][sublayer_idx] + (1 - self._beta_2) * sublayer_grad_squared
                    self._m_hat[layer_idx][sublayer_idx] = self._m[layer_idx][sublayer_idx] /(1 - (self._beta_1**self._epoch_num))
                    self._v_hat[layer_idx][sublayer_idx] = self._v[layer_idx][sublayer_idx] /(1 - (self._beta_2**self._epoch_num))
                    sublayer.assign_sub(self._lr * self._m_hat[layer_idx][sublayer_idx]
                                        /(tf.sqrt(self._v_hat[layer_idx][sublayer_idx])
                                           + self._lam/self._num_data))  # assign_sub for SGD update

        
        return self._running_loss / self._seen_batches

    def _init_adam_arrays(self):
        &#34;&#34;&#34;
        initialise the mean, second moment (sq_mean), deviation and trainable weights lists
        &#34;&#34;&#34;
        self._m = []
        self._m_hat = []
        self._v = []
        self._v_hat = []
        for layer_idx in range(len(self._base_model.layers)):
            layer = self._base_model.layers[layer_idx]
            if len(layer.trainable_variables) != 0:
                m_list = []
                m_hat_list = []
                v_list = []
                v_hat_list = []
                for var in layer.trainable_variables:
                    m_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    m_hat_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    v_list.append(tf.zeros(var.shape, dtype=tf.float32))
                    v_hat_list.append(tf.zeros(var.shape, dtype=tf.float32))
                self._m.append(m_list)
                self._m_hat.append(m_hat_list)
                self._v.append(v_list)
                self._v_hat.append(v_hat_list)
            else:
                self._m.append(None)
                self._m_hat.append(None)
                self._v.append(None)
                self._v_hat.append(None)
                
    def compile_extra_components(self, **kwargs):
        &#34;&#34;&#34;
            compiles components of subclasses
            Args:
                starting_model: this is the starting model for the inference method. It could be a pretrained model.
        &#34;&#34;&#34;
        self._lr = self._hyperparameters.lr
        self._batch_size = int(self._hyperparameters.batch_size)
        self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
        self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
        self._dataloader = (self._dataset.training_dataset()
                            .shuffle(self._dataset.training_dataset().cardinality())
                            .batch(self._batch_size))
        self._init_adam_arrays()
        self._data_iterator = iter(self._dataloader)
        self._n = 0
        self._lam = getattr(self._hyperparameters, &#39;lam&#39;, None) if hasattr(self._hyperparameters, &#39;lam&#39;) else self._lam
        self._num_data = self._dataset.train_data.cardinality()
        self._beta_1 = self._hyperparameters.beta_1
        self._beta_2 = self._hyperparameters.beta_2

        
    def result(self) -&gt; BayesianModel:
        self._mean = []
        self._var = []
        # for x in self._v:
        #     print(x.shape)
        model = BayesianModel(self._model_config)

        for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
            if len(layer.trainable_variables) != 0:
                mean = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
                mean = tf.reshape(tf.concat(mean, 0), (-1, 1))
                var = [tf.reshape(i, (-1, 1)) for i in self._v[layer_idx]]
                var = tf.reshape(tf.concat(var, 0), (-1, 1))

                tf_dist = tfp.distributions.Normal(loc=tf.reshape(mean, (-1,)), scale=tf.reshape(var, (-1,)))
                tf_dist = TensorflowProbabilityDistribution(
                    tf_dist
                )
                model.apply_distribution(tf_dist, layer_idx, layer_idx)
        return model
    def update_parameters_step(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Pyesian.optimizers.Optimizer.Optimizer</li>
<li>abc.ABC</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.optimizers.VADAM.compile_extra_components"><code class="name flex">
<span>def <span class="ident">compile_extra_components</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>compiles components of subclasses</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>starting_model</code></strong></dt>
<dd>this is the starting model for the inference method. It could be a pretrained model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compile_extra_components(self, **kwargs):
    &#34;&#34;&#34;
        compiles components of subclasses
        Args:
            starting_model: this is the starting model for the inference method. It could be a pretrained model.
    &#34;&#34;&#34;
    self._lr = self._hyperparameters.lr
    self._batch_size = int(self._hyperparameters.batch_size)
    self._base_model = tf.keras.models.clone_model(kwargs[&#34;starting_model&#34;])
    self._base_model.set_weights(kwargs[&#34;starting_model&#34;].get_weights())
    self._dataloader = (self._dataset.training_dataset()
                        .shuffle(self._dataset.training_dataset().cardinality())
                        .batch(self._batch_size))
    self._init_adam_arrays()
    self._data_iterator = iter(self._dataloader)
    self._n = 0
    self._lam = getattr(self._hyperparameters, &#39;lam&#39;, None) if hasattr(self._hyperparameters, &#39;lam&#39;) else self._lam
    self._num_data = self._dataset.train_data.cardinality()
    self._beta_1 = self._hyperparameters.beta_1
    self._beta_2 = self._hyperparameters.beta_2</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.VADAM.result"><code class="name flex">
<span>def <span class="ident">result</span></span>(<span>self) ‑> Pyesian.nn.BayesianModel.BayesianModel</span>
</code></dt>
<dd>
<div class="desc"><p>create a bayesian model at the stage of the training</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>BayesianModel</code></dt>
<dd>the bayesian model trained</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def result(self) -&gt; BayesianModel:
    self._mean = []
    self._var = []
    # for x in self._v:
    #     print(x.shape)
    model = BayesianModel(self._model_config)

    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        if len(layer.trainable_variables) != 0:
            mean = [tf.reshape(i, (-1, 1)) for i in layer.trainable_variables]
            mean = tf.reshape(tf.concat(mean, 0), (-1, 1))
            var = [tf.reshape(i, (-1, 1)) for i in self._v[layer_idx]]
            var = tf.reshape(tf.concat(var, 0), (-1, 1))

            tf_dist = tfp.distributions.Normal(loc=tf.reshape(mean, (-1,)), scale=tf.reshape(var, (-1,)))
            tf_dist = TensorflowProbabilityDistribution(
                tf_dist
            )
            model.apply_distribution(tf_dist, layer_idx, layer_idx)
    return model</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.VADAM.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, save_document_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs one step of the training</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>save_document_path</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>The path to save the losses during the training. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float</code></dt>
<dd>the loss value after the step</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, save_document_path = None):
    # get the sample and the label
    sample,label = next(self._data_iterator, (None,None))
    self._seen_batches += 1
    self._total_batches += 1
    # if the iterator reaches the end of the dataset, reinitialise the iterator
    if sample is None:
        # print(&#34;\n Loss after epoch %s: &#34;%(self._epoch_num), self._running_loss / self._seen_batches)
        self._data_iterator = iter(self._dataloader)
        self._seen_batches = 1
        self._running_loss = 0
        self._epoch_num += 1
        sample, label = next(self._data_iterator, (None, None))
    
    # Start by adding noise to the parameter 
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):
            eps = tf.random.normal(shape=sublayer.shape, mean = 0.0, stddev=1.0)
            sigma = 1/tf.sqrt(self._num_data * (self._v[layer_idx][sublayer_idx] + self._lam))
            #print(&#34;peturb norm: &#34;, tf.norm(sigma))
            sublayer.assign_add(eps * sigma)
        
    with tf.GradientTape(persistent=True) as tape:
        predictions = self._base_model(sample, training = True)
        loss = self._dataset.loss(reduction= &#39;none&#39;)(label, predictions)
        self._running_loss += tf.reduce_mean(loss)
        # save the loss if the path is specified
        if save_document_path != None:
            with open(save_document_path, &#34;a&#34;) as losses_file:
                losses_file.write(str(loss.numpy()))
    differentiation_variables = []
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        sublayer_differentiation_variables = []
        for sublayer, sublayer_idx in zip(layer.trainable_variables, range(len(layer.trainable_variables))):   
            sublayer_differentiation_variables.append(sublayer)
        differentiation_variables.append(sublayer_differentiation_variables)
    grad = tape.jacobian(loss, differentiation_variables)
    for layer, layer_idx in zip(self._base_model.layers, range(len(self._base_model.layers))):
        for sublayer, sublayer_idx, sublayer_grad in zip(layer.trainable_variables, range(len(layer.trainable_variables)), grad[layer_idx]):
             
            sublayer_grad_squared = sublayer_grad ** 2
            sublayer_grad_squared = tf.reduce_mean(sublayer_grad_squared, axis = 0)
            sublayer_grad = tf.reduce_mean(sublayer_grad, axis = 0)
            if sublayer_grad is not None:
                self._m[layer_idx][sublayer_idx] = self._beta_1 * self._m[layer_idx][sublayer_idx]
                self._m[layer_idx][sublayer_idx] += (1 - self._beta_1) * (sublayer_grad + (self._lam * sublayer/self._num_data))
                self._v[layer_idx][sublayer_idx] = self._beta_2 * self._v[layer_idx][sublayer_idx] + (1 - self._beta_2) * sublayer_grad_squared
                self._m_hat[layer_idx][sublayer_idx] = self._m[layer_idx][sublayer_idx] /(1 - (self._beta_1**self._epoch_num))
                self._v_hat[layer_idx][sublayer_idx] = self._v[layer_idx][sublayer_idx] /(1 - (self._beta_2**self._epoch_num))
                sublayer.assign_sub(self._lr * self._m_hat[layer_idx][sublayer_idx]
                                    /(tf.sqrt(self._v_hat[layer_idx][sublayer_idx])
                                       + self._lam/self._num_data))  # assign_sub for SGD update

    
    return self._running_loss / self._seen_batches</code></pre>
</details>
</dd>
<dt id="Pyesian.optimizers.VADAM.update_parameters_step"><code class="name flex">
<span>def <span class="ident">update_parameters_step</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>one step of updating the model parameters</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_parameters_step(self):
    pass</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Pyesian" href="../index.html">Pyesian</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="Pyesian.optimizers.hyperparameters" href="hyperparameters/index.html">Pyesian.optimizers.hyperparameters</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Pyesian.optimizers.ADAM" href="#Pyesian.optimizers.ADAM">ADAM</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.ADAM.compile_extra_components" href="#Pyesian.optimizers.ADAM.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.ADAM.result" href="#Pyesian.optimizers.ADAM.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.ADAM.step" href="#Pyesian.optimizers.ADAM.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.ADAM.update_parameters_step" href="#Pyesian.optimizers.ADAM.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.BBB" href="#Pyesian.optimizers.BBB">BBB</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.BBB.compile_extra_components" href="#Pyesian.optimizers.BBB.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.BBB.result" href="#Pyesian.optimizers.BBB.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.BBB.step" href="#Pyesian.optimizers.BBB.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.BBB.update_parameters_step" href="#Pyesian.optimizers.BBB.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.BSAM" href="#Pyesian.optimizers.BSAM">BSAM</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.BSAM.compile_extra_components" href="#Pyesian.optimizers.BSAM.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.BSAM.result" href="#Pyesian.optimizers.BSAM.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.BSAM.step" href="#Pyesian.optimizers.BSAM.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.BSAM.update_parameters_step" href="#Pyesian.optimizers.BSAM.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.HMC" href="#Pyesian.optimizers.HMC">HMC</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.HMC.compile_extra_components" href="#Pyesian.optimizers.HMC.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.HMC.result" href="#Pyesian.optimizers.HMC.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.HMC.step" href="#Pyesian.optimizers.HMC.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.HMC.train" href="#Pyesian.optimizers.HMC.train">train</a></code></li>
<li><code><a title="Pyesian.optimizers.HMC.update_parameters_step" href="#Pyesian.optimizers.HMC.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.Optimizer" href="#Pyesian.optimizers.Optimizer">Optimizer</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.Optimizer.compile" href="#Pyesian.optimizers.Optimizer.compile">compile</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.compile_extra_components" href="#Pyesian.optimizers.Optimizer.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.result" href="#Pyesian.optimizers.Optimizer.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.step" href="#Pyesian.optimizers.Optimizer.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.train" href="#Pyesian.optimizers.Optimizer.train">train</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.train_with_weights_and_biases" href="#Pyesian.optimizers.Optimizer.train_with_weights_and_biases">train_with_weights_and_biases</a></code></li>
<li><code><a title="Pyesian.optimizers.Optimizer.update_parameters_step" href="#Pyesian.optimizers.Optimizer.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.SGD" href="#Pyesian.optimizers.SGD">SGD</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.SGD.compile_extra_components" href="#Pyesian.optimizers.SGD.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.SGD.result" href="#Pyesian.optimizers.SGD.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.SGD.step" href="#Pyesian.optimizers.SGD.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.SGD.update_parameters_step" href="#Pyesian.optimizers.SGD.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.SGLD" href="#Pyesian.optimizers.SGLD">SGLD</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.SGLD.compile_extra_components" href="#Pyesian.optimizers.SGLD.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.SGLD.result" href="#Pyesian.optimizers.SGLD.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.SGLD.step" href="#Pyesian.optimizers.SGLD.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.SGLD.train" href="#Pyesian.optimizers.SGLD.train">train</a></code></li>
<li><code><a title="Pyesian.optimizers.SGLD.update_parameters_step" href="#Pyesian.optimizers.SGLD.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.SWAG" href="#Pyesian.optimizers.SWAG">SWAG</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.SWAG.compile_extra_components" href="#Pyesian.optimizers.SWAG.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.SWAG.result" href="#Pyesian.optimizers.SWAG.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.SWAG.step" href="#Pyesian.optimizers.SWAG.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.SWAG.update_parameters_step" href="#Pyesian.optimizers.SWAG.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Pyesian.optimizers.VADAM" href="#Pyesian.optimizers.VADAM">VADAM</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.optimizers.VADAM.compile_extra_components" href="#Pyesian.optimizers.VADAM.compile_extra_components">compile_extra_components</a></code></li>
<li><code><a title="Pyesian.optimizers.VADAM.result" href="#Pyesian.optimizers.VADAM.result">result</a></code></li>
<li><code><a title="Pyesian.optimizers.VADAM.step" href="#Pyesian.optimizers.VADAM.step">step</a></code></li>
<li><code><a title="Pyesian.optimizers.VADAM.update_parameters_step" href="#Pyesian.optimizers.VADAM.update_parameters_step">update_parameters_step</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>