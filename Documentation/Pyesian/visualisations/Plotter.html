<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Pyesian.visualisations.Plotter API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Pyesian.visualisations.Plotter</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import tensorflow as tf

from Pyesian.datasets import Dataset
from Pyesian.nn import BayesianModel
import matplotlib.pyplot as plt
import numpy as np
import scikitplot as skplt
from sklearn.decomposition import PCA
import os
import sklearn as sk



class Plotter:
    &#34;&#34;&#34;
        a class giving visualisation tools for performance analysis of a model over a dataset
        Args:
            model (BayesianModel): trained model that will make the predictions
            dataset (Dataset): the dataset on which to calculate the metrics
    &#34;&#34;&#34;
    def __init__(self, model: BayesianModel, dataset: Dataset):
        self._dataset = dataset
        self._model: BayesianModel = model
        self._nb_predictions: int = 0
        self._cached_samples : list = None
        self._cached_prediction: tf.Tensor = None
        self._cached_true_values: tf.Tensor = None
        self._cached_input: tf.tensor = None
        self._cached_data_type = None

    
    def _get_predictions(self, input, nb_boundaries, y_true, data_type):
        if (self._nb_predictions == nb_boundaries 
            and y_true.shape == self._cached_true_values.shape
            and data_type == self._cached_data_type):
            y_pred = self._cached_prediction
            if self._cached_prediction.shape[1] == 1 and self._dataset.likelihood_model == &#34;Classification&#34;:
                # in the very specific case of binary classification with one neuron output convert it to two output
                y_pred = tf.stack([1 - self._cached_prediction, self._cached_prediction], axis=1)
            return self._cached_samples, y_pred, self._cached_true_values, self._cached_input
        else:
            y_samples, y_pred = self._model.predict(input, nb_boundaries)  # pass in the x value
            self._nb_predictions = nb_boundaries
            self._cached_data_type = data_type
            self._cached_input = input
            self._cached_samples = tf.identity(y_samples)
            self._cached_prediction = tf.identity(y_pred)
            self._cached_true_values = tf.identity(y_true)
            if y_pred.shape[1] == 1 and self._dataset.likelihood_model == &#34;Classification&#34;:
                # in the very specific case of binary classification with one neuron output convert it to two output
                y_pred = tf.stack([1 - y_pred, y_pred], axis=1)
            return y_samples, y_pred, y_true, input

    def _plot_2d_uncertainty_area(self,
                                  x: tf.Tensor,
                                  y: tf.Tensor,
                                  base_matrix: tf.Tensor,
                                  granularity: float,
                                  n_samples: int,
                                  uncertainty_threshold: float,
                                  un_zoom_level: float):
        dim1, dim2, grid_x_augmented = self._extract_grid_x(x, base_matrix, granularity, un_zoom_level)
        _, predictions = self._model.predict(grid_x_augmented, n_samples)
        n_classes = tf.unique(y)[0].shape[0]
        colors = [(i / n_classes + 0.5 / n_classes) for i in range(n_classes)]
        for i in range(n_classes):
            plt.scatter(x[y == i][:, 0], x[y == i][:, 1], marker=&#39;o&#39;, cmap=colors[i], label=&#34;Class &#34; + str(i))
        if predictions.shape[1] == 1:
            # in the very specific case of binary classification with one neuron output convert it to two output
            predictions = tf.stack([1 - predictions, predictions], axis=1)
        predictions_max = tf.math.reduce_max(predictions, axis=1)
        uncertainty_area = tf.cast(predictions_max &lt; uncertainty_threshold, dtype=tf.float32)
        uncertainty_area = tf.reshape(uncertainty_area, (dim1.shape[0], dim1.shape[1]))
        plt.contourf(dim1, dim2, uncertainty_area, [0.9, 1.1], colors=[&#34;orange&#34;], alpha=0.5)
        plt.legend()
        plt.title(&#34;Uncertainty area with threshold &#34; + str(uncertainty_threshold))

    def _get_x_y(self, n_samples=100, data_type=&#34;test&#34;):
        tf_dataset = self._dataset.valid_data
        if data_type == &#34;test&#34;:
            tf_dataset = self._dataset.test_data
        elif data_type == &#34;train&#34;:
            tf_dataset = self._dataset.train_data
        x,y_true = next(iter(tf_dataset.batch(n_samples)))
        return x,y_true

    def _extract_x_y_from_dataset(self, dimension=2, n_samples=100, data_type=&#34;test&#34;) -&gt; (tf.Tensor, tf.Tensor):
        x, y = self._get_x_y(n_samples, data_type)

        if x.shape[1] &gt; dimension:
            print(&#34;Dimension &#34;, len(x.shape[1]), &#34; is not right.&#34;)
            print(&#34;Will apply PCA to reduce to dimension &#34;, dimension)
            base_matrix = tf.pca(x, dimension, dtype=x.dtype)
            return tf.linalg.matmul(x, base_matrix), y, base_matrix
        elif x.shape[1] &lt; dimension:
            raise ValueError(&#34;Dimension &#34;, x.shape[1], &#34; is inferior to given dimension&#34;)
        return x, y, tf.eye(dimension, dtype=x.dtype)

    def _plot_2d_decision_boundary(self,
                                   x: tf.Tensor,
                                   y: tf.Tensor,
                                   base_matrix: tf.Tensor,
                                   dimension=2,
                                   granularity=1e-2,
                                   n_boundaries=10,
                                   un_zoom_level=0.2):
        dim1, dim2, grid_x_augmented = self._extract_grid_x(x, base_matrix, granularity, un_zoom_level)
        prediction_samples, _ = self._model.predict(grid_x_augmented, n_boundaries)
        plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1], marker=&#39;o&#39;, c=&#34;blue&#34;, label=&#34;Class 0&#34;)
        plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1], marker=&#39;x&#39;, c=&#34;red&#34;, label=&#34;Class 1&#34;)
        for pred in prediction_samples:
            pred = tf.reshape(pred[:, 0], dim1.shape)
            plt.contour(dim1, dim2, pred, [0.5], colors=[&#34;red&#34;])
        plt.legend()
        plt.title(&#34;Multiple Decision Boundaries N=&#34; + str(n_boundaries))

    def _extract_grid_x(self, x, base_matrix, granularity, un_zoom_level: float):
        max_features = tf.math.reduce_max(x, axis=0)
        min_features = tf.math.reduce_min(x, axis=0)
        size1 = (max_features[0] - min_features[0])
        size2 = (max_features[1] - min_features[1])
        dim1 = tf.range(min_features[0] - (un_zoom_level/2) * size1,
                        max_features[0] + (un_zoom_level/2) * size1,
                        granularity * (max_features[0] - min_features[0] + un_zoom_level * size1))
        dim2 = tf.range(min_features[1] - (un_zoom_level/2) * size2,
                        max_features[1] + (un_zoom_level/2) * size2,
                        granularity * (max_features[1] - min_features[1] + un_zoom_level * size2))
        dim1, dim2 = tf.meshgrid(dim1, dim2, indexing=&#39;ij&#39;)
        grid_x = tf.stack([tf.reshape(dim1, (-1)), tf.reshape(dim2, (-1))], axis=1)
        grid_x_augmented = tf.linalg.matmul(grid_x, tf.transpose(base_matrix))
        return dim1, dim2, grid_x_augmented
    
    def roc_one_vs_rest(self, n_samples = 100, label_of_interest: int = 0, n_boundaries = 10, data_type = &#34;test&#34;):
        &#34;&#34;&#34;plots the ROC curve in a one vs the rest fashion

        Args:
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            label_of_interest (int, optional): The label that will be opposed to the other for ROC calculation. Defaults to 0.
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation. Defaults to 10.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.

        Raises:
            ValueError: when the method is called for other than a classification problem
        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;ROC can only be plotted for Classification&#34;)  
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        one_hot_y_true = tf.one_hot(y_true, y_pred.shape[1])
        display = sk.metrics.RocCurveDisplay.from_predictions(
            one_hot_y_true[:, label_of_interest],
            y_pred[:, label_of_interest],
            name=f&#34;class {label_of_interest} vs the rest&#34;,
            color=&#34;blue&#34;,
            plot_chance_level=True,
        )
        _ = display.ax_.set(
            xlabel=&#34;False Positive Rate&#34;,
            ylabel=&#34;True Positive Rate&#34;,
            title=&#34;ROC curve One-vs-Rest&#34;,
        )
        plt.show()

   
    def plot_decision_boundaries(self, dimension=2, granularity=1e-2, n_boundaries=30, n_samples=100,
                                 data_type=&#34;test&#34;, un_zoom_level=0.2, save_path=None):
        &#34;&#34;&#34;plots the n_boundaries decision boundaries for a classification task

        Args:
            dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
            granularity (int, optional): The precision of the plot.
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Decision boundary can only be plotted for Classification&#34;)
        x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                           data_type=data_type)
        if dimension == 2:
            self._plot_2d_decision_boundary(x, y, base_matrix, dimension=2, granularity= granularity, 
                                            n_boundaries=10, un_zoom_level=un_zoom_level)
            self._save(save_path, &#34;decision_boundaries&#34;) if save_path else plt.show()
        else:
            raise ValueError(&#34;Decision boundary can only be plotted in 2 dimensions&#34;)


    def plot_uncertainty_area(self,
                              dimension=2,
                              granularity: float = 1e-2,
                              n_samples=100, data_type=&#34;test&#34;, uncertainty_threshold=0.8,
                              un_zoom_level=0.2,
                              save_path=None):
        &#34;&#34;&#34;plots the uncertainty area for a classification task

        Args:
            dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
            granularity (float, optional): The precision of the plot. Defaults to 1e-2.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            uncertainty_threshold (float, optional): the threshold below which we consider the prediction uncertain. Defaults to 0.8.
            un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem
        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Uncertainty area can only be plotted for Classification&#34;)
        x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                           data_type=data_type)
        if dimension == 2:
            self._plot_2d_uncertainty_area(x, y, base_matrix, granularity, n_samples, uncertainty_threshold,
                                           un_zoom_level)
            self._save(save_path, &#34;uncertainty_area&#34;) if save_path else plt.show()


    def regression_uncertainty(self, n_boundaries = 30, n_samples = 100, data_type=&#34;test&#34;, save_path=None) -&gt; tuple:
        &#34;&#34;&#34;Plots the epistemic uncertainty for a regression problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a regression dataset

        &#34;&#34;&#34;
        if self._dataset.likelihood_model == &#34;Regression&#34;:
            x,y_true = self._get_x_y(n_samples, data_type)
            y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
            variance = np.var(y_samples, axis=0)
            err = np.mean(np.sqrt(variance), axis = 1)
            pred_dev = np.mean((y_pred.numpy()-y_true.numpy()), axis = 1)
            # uncertainty
            plt.figure(figsize=(10, 5))
            plt.hlines([0], 0, len(err))
            plt.plot(range(len(err)), pred_dev-err, label=&#39;Epistemic Lower&#39;, alpha=0.5)
            plt.scatter(range(len(err)), pred_dev, label=&#39;Averaged deviation&#39;, alpha=0.5, c=&#34;k&#34;)
            plt.plot(range(len(err)), pred_dev+err, label=&#39;Epistemic Upper&#39;, alpha=0.5)
            plt.legend()
            plt.title(&#39;Epistemic Uncertainty&#39;)
            plt.ylabel(&#39;Pred-True difference&#39;)
            self._save(save_path, &#34;epistemic_uncertainty&#34;) if save_path else plt.show()
        else:
            raise ValueError(&#34;regression uncertainty cannot be computed for other than regression problems&#34;)

    

    def confusion_matrix(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;,  save_path=None):
        &#34;&#34;&#34;Plots the confusion matrix for a classification problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Confusion matrix cannot be computed for other than classification problems&#34;)

        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
        y_pred_labels = tf.argmax(y_pred, axis=1)
        y_true = tf.reshape(y_true, y_pred_labels.shape)
        skplt.metrics.plot_confusion_matrix(y_true, y_pred_labels, normalize=True, title = &#39;Confusion Matrix&#39;)
        self._save(save_path, &#34;confusion_matrix&#34;) if save_path else plt.show()
            

    def compare_prediction_to_target(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
        &#34;&#34;&#34;
        Plots a comparision between the true values or labels and the predicitions for both classification and regression problems.

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
        &#34;&#34;&#34;
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        if self._dataset.likelihood_model == &#34;Regression&#34;:
            y_true = tf.reshape(y_true, y_pred.shape)
            if y_true.shape[1] == 1:
                plt.figure(figsize=(10, 5))
                plt.scatter(range(len(y_true)), y_true, label=&#39;True Values&#39;, alpha=0.5)
                plt.scatter(range(len(y_pred)), y_pred, label=&#39;Predicted Mean&#39;, alpha=0.5)
                plt.legend()
                plt.title(&#39;True vs Predicted Values&#39;)
                plt.xlabel(&#39;Sample Index&#39;)
                plt.ylabel(&#39;Output&#39;)
                self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()
        else:
            y_pred_labels = tf.argmax(y_pred, axis=1)
            x_2d = tf.reshape(x, (x.shape[0], -1))
            if x_2d.shape[1] == 2:
                self._compare_prediction_to_target_2d(x_2d, y_true, y_pred_labels)
            else:
                if(x_2d.shape[1]&gt;=3):
                    x_pca = PCA(n_components=3).fit_transform(x_2d)
                    self._compare_prediction_to_target_3d(x_pca, y_true, y_pred_labels, save_path=save_path)
                else:
                    x_pca = PCA(n_components=2).fit_transform(x_2d)
                    self._compare_prediction_to_target_2d(x_pca, y_true, y_pred_labels, save_path=save_path)


    def _compare_prediction_to_target_2d(self, x_pca, y_true, y_pred, save_path=None):
        fig, (ax_true, ax_pred) = plt.subplots(2, figsize=(12, 8))
        scatter_true = ax_true.scatter(x_pca[:, -2], x_pca[:, -1], c=y_true, s=5)
        legend_plt_true = ax_true.legend(*scatter_true.legend_elements(), loc=&#34;lower left&#34;, title=&#34;Digits&#34;)
        ax_true.add_artist(legend_plt_true)
        scatter_pred = ax_pred.scatter(x_pca[:, -2], x_pca[:, -1], c=y_pred, s=5)
        legend_plt_pred = ax_pred.legend(*scatter_pred.legend_elements(), loc=&#34;lower left&#34;, title=&#34;Digits&#34;)
        ax_pred.add_artist(legend_plt_pred)
        ax_true.set_title(&#39;First Two Dimensions of Projected True Data After Applying PCA&#39;)
        ax_pred.set_title(&#39;First Two Dimensions of Projected Predicted Data After Applying PCA&#39;)
        self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()
        
    def _compare_prediction_to_target_3d(self, x_pca, y_true, y_pred, save_path=None):
        fig = plt.figure(figsize=plt.figaspect(0.5))
        ax_true = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;)
        plt_3d_true = ax_true.scatter3D(x_pca[:, -3], x_pca[:, -2], x_pca[:, -1], c=y_true, s=1)
        fig.colorbar(plt_3d_true, shrink=0.5)
        
        ax_pred = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;)
        plt_3d_pred = ax_pred.scatter3D(x_pca[:, -3], x_pca[:, -2], x_pca[:, -1], c=y_pred, s=1)
        fig.colorbar(plt_3d_pred, shrink=0.5)
        
        plt.title(&#39;First Three Dimensions of Projected True Data (left) VS Predicted Data (right) After Applying PCA&#39;)
        self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()

    def entropy(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
        &#34;&#34;&#34;Plots the entropy over the prediciton probabilities for a classification problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        if self._dataset.likelihood_model == &#34;Classification&#34;:
            entropies = []
            for probabilities in y_pred:
                entropies.append(-1*np.sum(probabilities*np.log(probabilities+1e-5)))
            entropies = np.sort(np.nan_to_num(entropies))
            plt.plot(range(len(y_true)), entropies)
            plt.title(&#39;Entropies for each input&#39;)
            plt.xlabel(&#39;Sample Index&#39;)
            plt.ylabel(&#39;entropy&#39;)
            self._save(save_path, &#34;entropy&#34;) if save_path else plt.show()
        else:
            raise Exception(&#34;Entropy is only available for classification&#34;)
        


    def learning_diagnostics(self, loss_file: str, save_path=None):
        &#34;&#34;&#34;plots the evolution of the loss function during the training. The losses should be saved inside a file.

        Args:
            loss_file (str): The path to the file with the training losses
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
        &#34;&#34;&#34;
        if loss_file != None:
            losses = np.loadtxt(loss_file)
            plt.plot(losses)
            plt.title(&#34;Training Loss&#34;)
            plt.xlabel(&#34;Iterations&#34;)
            plt.ylabel(&#34;Loss&#34;)
            plt.legend()
            self._save(save_path, &#34;learning_diagnostics&#34;) if save_path else plt.show()


    def _save(self, save_path, name):
        directory = os.path.join(save_path,&#34;report&#34;)
        plots = os.path.join(directory, &#34;plots&#34;)
        os.makedirs(directory, exist_ok=True)
        os.makedirs(plots, exist_ok=True)
        plt.savefig(os.path.join(plots, name + &#34;.png&#34;))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Pyesian.visualisations.Plotter.Plotter"><code class="flex name class">
<span>class <span class="ident">Plotter</span></span>
<span>(</span><span>model: Pyesian.nn.BayesianModel.BayesianModel, dataset: Pyesian.datasets.Dataset.Dataset)</span>
</code></dt>
<dd>
<div class="desc"><p>a class giving visualisation tools for performance analysis of a model over a dataset</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>BayesianModel</code></dt>
<dd>trained model that will make the predictions</dd>
<dt><strong><code>dataset</code></strong> :&ensp;<code>Dataset</code></dt>
<dd>the dataset on which to calculate the metrics</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Plotter:
    &#34;&#34;&#34;
        a class giving visualisation tools for performance analysis of a model over a dataset
        Args:
            model (BayesianModel): trained model that will make the predictions
            dataset (Dataset): the dataset on which to calculate the metrics
    &#34;&#34;&#34;
    def __init__(self, model: BayesianModel, dataset: Dataset):
        self._dataset = dataset
        self._model: BayesianModel = model
        self._nb_predictions: int = 0
        self._cached_samples : list = None
        self._cached_prediction: tf.Tensor = None
        self._cached_true_values: tf.Tensor = None
        self._cached_input: tf.tensor = None
        self._cached_data_type = None

    
    def _get_predictions(self, input, nb_boundaries, y_true, data_type):
        if (self._nb_predictions == nb_boundaries 
            and y_true.shape == self._cached_true_values.shape
            and data_type == self._cached_data_type):
            y_pred = self._cached_prediction
            if self._cached_prediction.shape[1] == 1 and self._dataset.likelihood_model == &#34;Classification&#34;:
                # in the very specific case of binary classification with one neuron output convert it to two output
                y_pred = tf.stack([1 - self._cached_prediction, self._cached_prediction], axis=1)
            return self._cached_samples, y_pred, self._cached_true_values, self._cached_input
        else:
            y_samples, y_pred = self._model.predict(input, nb_boundaries)  # pass in the x value
            self._nb_predictions = nb_boundaries
            self._cached_data_type = data_type
            self._cached_input = input
            self._cached_samples = tf.identity(y_samples)
            self._cached_prediction = tf.identity(y_pred)
            self._cached_true_values = tf.identity(y_true)
            if y_pred.shape[1] == 1 and self._dataset.likelihood_model == &#34;Classification&#34;:
                # in the very specific case of binary classification with one neuron output convert it to two output
                y_pred = tf.stack([1 - y_pred, y_pred], axis=1)
            return y_samples, y_pred, y_true, input

    def _plot_2d_uncertainty_area(self,
                                  x: tf.Tensor,
                                  y: tf.Tensor,
                                  base_matrix: tf.Tensor,
                                  granularity: float,
                                  n_samples: int,
                                  uncertainty_threshold: float,
                                  un_zoom_level: float):
        dim1, dim2, grid_x_augmented = self._extract_grid_x(x, base_matrix, granularity, un_zoom_level)
        _, predictions = self._model.predict(grid_x_augmented, n_samples)
        n_classes = tf.unique(y)[0].shape[0]
        colors = [(i / n_classes + 0.5 / n_classes) for i in range(n_classes)]
        for i in range(n_classes):
            plt.scatter(x[y == i][:, 0], x[y == i][:, 1], marker=&#39;o&#39;, cmap=colors[i], label=&#34;Class &#34; + str(i))
        if predictions.shape[1] == 1:
            # in the very specific case of binary classification with one neuron output convert it to two output
            predictions = tf.stack([1 - predictions, predictions], axis=1)
        predictions_max = tf.math.reduce_max(predictions, axis=1)
        uncertainty_area = tf.cast(predictions_max &lt; uncertainty_threshold, dtype=tf.float32)
        uncertainty_area = tf.reshape(uncertainty_area, (dim1.shape[0], dim1.shape[1]))
        plt.contourf(dim1, dim2, uncertainty_area, [0.9, 1.1], colors=[&#34;orange&#34;], alpha=0.5)
        plt.legend()
        plt.title(&#34;Uncertainty area with threshold &#34; + str(uncertainty_threshold))

    def _get_x_y(self, n_samples=100, data_type=&#34;test&#34;):
        tf_dataset = self._dataset.valid_data
        if data_type == &#34;test&#34;:
            tf_dataset = self._dataset.test_data
        elif data_type == &#34;train&#34;:
            tf_dataset = self._dataset.train_data
        x,y_true = next(iter(tf_dataset.batch(n_samples)))
        return x,y_true

    def _extract_x_y_from_dataset(self, dimension=2, n_samples=100, data_type=&#34;test&#34;) -&gt; (tf.Tensor, tf.Tensor):
        x, y = self._get_x_y(n_samples, data_type)

        if x.shape[1] &gt; dimension:
            print(&#34;Dimension &#34;, len(x.shape[1]), &#34; is not right.&#34;)
            print(&#34;Will apply PCA to reduce to dimension &#34;, dimension)
            base_matrix = tf.pca(x, dimension, dtype=x.dtype)
            return tf.linalg.matmul(x, base_matrix), y, base_matrix
        elif x.shape[1] &lt; dimension:
            raise ValueError(&#34;Dimension &#34;, x.shape[1], &#34; is inferior to given dimension&#34;)
        return x, y, tf.eye(dimension, dtype=x.dtype)

    def _plot_2d_decision_boundary(self,
                                   x: tf.Tensor,
                                   y: tf.Tensor,
                                   base_matrix: tf.Tensor,
                                   dimension=2,
                                   granularity=1e-2,
                                   n_boundaries=10,
                                   un_zoom_level=0.2):
        dim1, dim2, grid_x_augmented = self._extract_grid_x(x, base_matrix, granularity, un_zoom_level)
        prediction_samples, _ = self._model.predict(grid_x_augmented, n_boundaries)
        plt.scatter(x[y == 0][:, 0], x[y == 0][:, 1], marker=&#39;o&#39;, c=&#34;blue&#34;, label=&#34;Class 0&#34;)
        plt.scatter(x[y == 1][:, 0], x[y == 1][:, 1], marker=&#39;x&#39;, c=&#34;red&#34;, label=&#34;Class 1&#34;)
        for pred in prediction_samples:
            pred = tf.reshape(pred[:, 0], dim1.shape)
            plt.contour(dim1, dim2, pred, [0.5], colors=[&#34;red&#34;])
        plt.legend()
        plt.title(&#34;Multiple Decision Boundaries N=&#34; + str(n_boundaries))

    def _extract_grid_x(self, x, base_matrix, granularity, un_zoom_level: float):
        max_features = tf.math.reduce_max(x, axis=0)
        min_features = tf.math.reduce_min(x, axis=0)
        size1 = (max_features[0] - min_features[0])
        size2 = (max_features[1] - min_features[1])
        dim1 = tf.range(min_features[0] - (un_zoom_level/2) * size1,
                        max_features[0] + (un_zoom_level/2) * size1,
                        granularity * (max_features[0] - min_features[0] + un_zoom_level * size1))
        dim2 = tf.range(min_features[1] - (un_zoom_level/2) * size2,
                        max_features[1] + (un_zoom_level/2) * size2,
                        granularity * (max_features[1] - min_features[1] + un_zoom_level * size2))
        dim1, dim2 = tf.meshgrid(dim1, dim2, indexing=&#39;ij&#39;)
        grid_x = tf.stack([tf.reshape(dim1, (-1)), tf.reshape(dim2, (-1))], axis=1)
        grid_x_augmented = tf.linalg.matmul(grid_x, tf.transpose(base_matrix))
        return dim1, dim2, grid_x_augmented
    
    def roc_one_vs_rest(self, n_samples = 100, label_of_interest: int = 0, n_boundaries = 10, data_type = &#34;test&#34;):
        &#34;&#34;&#34;plots the ROC curve in a one vs the rest fashion

        Args:
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            label_of_interest (int, optional): The label that will be opposed to the other for ROC calculation. Defaults to 0.
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation. Defaults to 10.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.

        Raises:
            ValueError: when the method is called for other than a classification problem
        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;ROC can only be plotted for Classification&#34;)  
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        one_hot_y_true = tf.one_hot(y_true, y_pred.shape[1])
        display = sk.metrics.RocCurveDisplay.from_predictions(
            one_hot_y_true[:, label_of_interest],
            y_pred[:, label_of_interest],
            name=f&#34;class {label_of_interest} vs the rest&#34;,
            color=&#34;blue&#34;,
            plot_chance_level=True,
        )
        _ = display.ax_.set(
            xlabel=&#34;False Positive Rate&#34;,
            ylabel=&#34;True Positive Rate&#34;,
            title=&#34;ROC curve One-vs-Rest&#34;,
        )
        plt.show()

   
    def plot_decision_boundaries(self, dimension=2, granularity=1e-2, n_boundaries=30, n_samples=100,
                                 data_type=&#34;test&#34;, un_zoom_level=0.2, save_path=None):
        &#34;&#34;&#34;plots the n_boundaries decision boundaries for a classification task

        Args:
            dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
            granularity (int, optional): The precision of the plot.
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Decision boundary can only be plotted for Classification&#34;)
        x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                           data_type=data_type)
        if dimension == 2:
            self._plot_2d_decision_boundary(x, y, base_matrix, dimension=2, granularity= granularity, 
                                            n_boundaries=10, un_zoom_level=un_zoom_level)
            self._save(save_path, &#34;decision_boundaries&#34;) if save_path else plt.show()
        else:
            raise ValueError(&#34;Decision boundary can only be plotted in 2 dimensions&#34;)


    def plot_uncertainty_area(self,
                              dimension=2,
                              granularity: float = 1e-2,
                              n_samples=100, data_type=&#34;test&#34;, uncertainty_threshold=0.8,
                              un_zoom_level=0.2,
                              save_path=None):
        &#34;&#34;&#34;plots the uncertainty area for a classification task

        Args:
            dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
            granularity (float, optional): The precision of the plot. Defaults to 1e-2.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            uncertainty_threshold (float, optional): the threshold below which we consider the prediction uncertain. Defaults to 0.8.
            un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem
        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Uncertainty area can only be plotted for Classification&#34;)
        x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                           data_type=data_type)
        if dimension == 2:
            self._plot_2d_uncertainty_area(x, y, base_matrix, granularity, n_samples, uncertainty_threshold,
                                           un_zoom_level)
            self._save(save_path, &#34;uncertainty_area&#34;) if save_path else plt.show()


    def regression_uncertainty(self, n_boundaries = 30, n_samples = 100, data_type=&#34;test&#34;, save_path=None) -&gt; tuple:
        &#34;&#34;&#34;Plots the epistemic uncertainty for a regression problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a regression dataset

        &#34;&#34;&#34;
        if self._dataset.likelihood_model == &#34;Regression&#34;:
            x,y_true = self._get_x_y(n_samples, data_type)
            y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
            variance = np.var(y_samples, axis=0)
            err = np.mean(np.sqrt(variance), axis = 1)
            pred_dev = np.mean((y_pred.numpy()-y_true.numpy()), axis = 1)
            # uncertainty
            plt.figure(figsize=(10, 5))
            plt.hlines([0], 0, len(err))
            plt.plot(range(len(err)), pred_dev-err, label=&#39;Epistemic Lower&#39;, alpha=0.5)
            plt.scatter(range(len(err)), pred_dev, label=&#39;Averaged deviation&#39;, alpha=0.5, c=&#34;k&#34;)
            plt.plot(range(len(err)), pred_dev+err, label=&#39;Epistemic Upper&#39;, alpha=0.5)
            plt.legend()
            plt.title(&#39;Epistemic Uncertainty&#39;)
            plt.ylabel(&#39;Pred-True difference&#39;)
            self._save(save_path, &#34;epistemic_uncertainty&#34;) if save_path else plt.show()
        else:
            raise ValueError(&#34;regression uncertainty cannot be computed for other than regression problems&#34;)

    

    def confusion_matrix(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;,  save_path=None):
        &#34;&#34;&#34;Plots the confusion matrix for a classification problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        if self._dataset.likelihood_model != &#34;Classification&#34;:
            raise ValueError(&#34;Confusion matrix cannot be computed for other than classification problems&#34;)

        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
        y_pred_labels = tf.argmax(y_pred, axis=1)
        y_true = tf.reshape(y_true, y_pred_labels.shape)
        skplt.metrics.plot_confusion_matrix(y_true, y_pred_labels, normalize=True, title = &#39;Confusion Matrix&#39;)
        self._save(save_path, &#34;confusion_matrix&#34;) if save_path else plt.show()
            

    def compare_prediction_to_target(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
        &#34;&#34;&#34;
        Plots a comparision between the true values or labels and the predicitions for both classification and regression problems.

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
        &#34;&#34;&#34;
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        if self._dataset.likelihood_model == &#34;Regression&#34;:
            y_true = tf.reshape(y_true, y_pred.shape)
            if y_true.shape[1] == 1:
                plt.figure(figsize=(10, 5))
                plt.scatter(range(len(y_true)), y_true, label=&#39;True Values&#39;, alpha=0.5)
                plt.scatter(range(len(y_pred)), y_pred, label=&#39;Predicted Mean&#39;, alpha=0.5)
                plt.legend()
                plt.title(&#39;True vs Predicted Values&#39;)
                plt.xlabel(&#39;Sample Index&#39;)
                plt.ylabel(&#39;Output&#39;)
                self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()
        else:
            y_pred_labels = tf.argmax(y_pred, axis=1)
            x_2d = tf.reshape(x, (x.shape[0], -1))
            if x_2d.shape[1] == 2:
                self._compare_prediction_to_target_2d(x_2d, y_true, y_pred_labels)
            else:
                if(x_2d.shape[1]&gt;=3):
                    x_pca = PCA(n_components=3).fit_transform(x_2d)
                    self._compare_prediction_to_target_3d(x_pca, y_true, y_pred_labels, save_path=save_path)
                else:
                    x_pca = PCA(n_components=2).fit_transform(x_2d)
                    self._compare_prediction_to_target_2d(x_pca, y_true, y_pred_labels, save_path=save_path)


    def _compare_prediction_to_target_2d(self, x_pca, y_true, y_pred, save_path=None):
        fig, (ax_true, ax_pred) = plt.subplots(2, figsize=(12, 8))
        scatter_true = ax_true.scatter(x_pca[:, -2], x_pca[:, -1], c=y_true, s=5)
        legend_plt_true = ax_true.legend(*scatter_true.legend_elements(), loc=&#34;lower left&#34;, title=&#34;Digits&#34;)
        ax_true.add_artist(legend_plt_true)
        scatter_pred = ax_pred.scatter(x_pca[:, -2], x_pca[:, -1], c=y_pred, s=5)
        legend_plt_pred = ax_pred.legend(*scatter_pred.legend_elements(), loc=&#34;lower left&#34;, title=&#34;Digits&#34;)
        ax_pred.add_artist(legend_plt_pred)
        ax_true.set_title(&#39;First Two Dimensions of Projected True Data After Applying PCA&#39;)
        ax_pred.set_title(&#39;First Two Dimensions of Projected Predicted Data After Applying PCA&#39;)
        self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()
        
    def _compare_prediction_to_target_3d(self, x_pca, y_true, y_pred, save_path=None):
        fig = plt.figure(figsize=plt.figaspect(0.5))
        ax_true = fig.add_subplot(1, 2, 1, projection=&#39;3d&#39;)
        plt_3d_true = ax_true.scatter3D(x_pca[:, -3], x_pca[:, -2], x_pca[:, -1], c=y_true, s=1)
        fig.colorbar(plt_3d_true, shrink=0.5)
        
        ax_pred = fig.add_subplot(1, 2, 2, projection=&#39;3d&#39;)
        plt_3d_pred = ax_pred.scatter3D(x_pca[:, -3], x_pca[:, -2], x_pca[:, -1], c=y_pred, s=1)
        fig.colorbar(plt_3d_pred, shrink=0.5)
        
        plt.title(&#39;First Three Dimensions of Projected True Data (left) VS Predicted Data (right) After Applying PCA&#39;)
        self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()

    def entropy(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
        &#34;&#34;&#34;Plots the entropy over the prediciton probabilities for a classification problem

        Args:
            n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
            n_samples (int, optional): number of samples from the dataset. Defaults to 100.
            data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

        Raises:
            ValueError: when the method is called for other than a classification problem

        &#34;&#34;&#34;
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
        if self._dataset.likelihood_model == &#34;Classification&#34;:
            entropies = []
            for probabilities in y_pred:
                entropies.append(-1*np.sum(probabilities*np.log(probabilities+1e-5)))
            entropies = np.sort(np.nan_to_num(entropies))
            plt.plot(range(len(y_true)), entropies)
            plt.title(&#39;Entropies for each input&#39;)
            plt.xlabel(&#39;Sample Index&#39;)
            plt.ylabel(&#39;entropy&#39;)
            self._save(save_path, &#34;entropy&#34;) if save_path else plt.show()
        else:
            raise Exception(&#34;Entropy is only available for classification&#34;)
        


    def learning_diagnostics(self, loss_file: str, save_path=None):
        &#34;&#34;&#34;plots the evolution of the loss function during the training. The losses should be saved inside a file.

        Args:
            loss_file (str): The path to the file with the training losses
            save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
        &#34;&#34;&#34;
        if loss_file != None:
            losses = np.loadtxt(loss_file)
            plt.plot(losses)
            plt.title(&#34;Training Loss&#34;)
            plt.xlabel(&#34;Iterations&#34;)
            plt.ylabel(&#34;Loss&#34;)
            plt.legend()
            self._save(save_path, &#34;learning_diagnostics&#34;) if save_path else plt.show()


    def _save(self, save_path, name):
        directory = os.path.join(save_path,&#34;report&#34;)
        plots = os.path.join(directory, &#34;plots&#34;)
        os.makedirs(directory, exist_ok=True)
        os.makedirs(plots, exist_ok=True)
        plt.savefig(os.path.join(plots, name + &#34;.png&#34;))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Pyesian.visualisations.Plotter.Plotter.compare_prediction_to_target"><code class="name flex">
<span>def <span class="ident">compare_prediction_to_target</span></span>(<span>self, n_boundaries=30, n_samples=100, data_type='test', save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots a comparision between the true values or labels and the predicitions for both classification and regression problems.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compare_prediction_to_target(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
    &#34;&#34;&#34;
    Plots a comparision between the true values or labels and the predicitions for both classification and regression problems.

    Args:
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
    &#34;&#34;&#34;
    x,y_true = self._get_x_y(n_samples, data_type)
    y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
    if self._dataset.likelihood_model == &#34;Regression&#34;:
        y_true = tf.reshape(y_true, y_pred.shape)
        if y_true.shape[1] == 1:
            plt.figure(figsize=(10, 5))
            plt.scatter(range(len(y_true)), y_true, label=&#39;True Values&#39;, alpha=0.5)
            plt.scatter(range(len(y_pred)), y_pred, label=&#39;Predicted Mean&#39;, alpha=0.5)
            plt.legend()
            plt.title(&#39;True vs Predicted Values&#39;)
            plt.xlabel(&#39;Sample Index&#39;)
            plt.ylabel(&#39;Output&#39;)
            self._save(save_path, &#34;comparison_pred_true&#34;) if save_path else plt.show()
    else:
        y_pred_labels = tf.argmax(y_pred, axis=1)
        x_2d = tf.reshape(x, (x.shape[0], -1))
        if x_2d.shape[1] == 2:
            self._compare_prediction_to_target_2d(x_2d, y_true, y_pred_labels)
        else:
            if(x_2d.shape[1]&gt;=3):
                x_pca = PCA(n_components=3).fit_transform(x_2d)
                self._compare_prediction_to_target_3d(x_pca, y_true, y_pred_labels, save_path=save_path)
            else:
                x_pca = PCA(n_components=2).fit_transform(x_2d)
                self._compare_prediction_to_target_2d(x_pca, y_true, y_pred_labels, save_path=save_path)</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.confusion_matrix"><code class="name flex">
<span>def <span class="ident">confusion_matrix</span></span>(<span>self, n_boundaries=30, n_samples=100, data_type='test', save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the confusion matrix for a classification problem</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a classification problem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def confusion_matrix(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;,  save_path=None):
    &#34;&#34;&#34;Plots the confusion matrix for a classification problem

    Args:
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

    Raises:
        ValueError: when the method is called for other than a classification problem

    &#34;&#34;&#34;
    if self._dataset.likelihood_model != &#34;Classification&#34;:
        raise ValueError(&#34;Confusion matrix cannot be computed for other than classification problems&#34;)

    x,y_true = self._get_x_y(n_samples, data_type)
    y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
    y_pred_labels = tf.argmax(y_pred, axis=1)
    y_true = tf.reshape(y_true, y_pred_labels.shape)
    skplt.metrics.plot_confusion_matrix(y_true, y_pred_labels, normalize=True, title = &#39;Confusion Matrix&#39;)
    self._save(save_path, &#34;confusion_matrix&#34;) if save_path else plt.show()</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.entropy"><code class="name flex">
<span>def <span class="ident">entropy</span></span>(<span>self, n_boundaries=30, n_samples=100, data_type='test', save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the entropy over the prediciton probabilities for a classification problem</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a classification problem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def entropy(self, n_boundaries = 30, n_samples=100, data_type=&#34;test&#34;, save_path=None):
    &#34;&#34;&#34;Plots the entropy over the prediciton probabilities for a classification problem

    Args:
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

    Raises:
        ValueError: when the method is called for other than a classification problem

    &#34;&#34;&#34;
    x,y_true = self._get_x_y(n_samples, data_type)
    y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
    if self._dataset.likelihood_model == &#34;Classification&#34;:
        entropies = []
        for probabilities in y_pred:
            entropies.append(-1*np.sum(probabilities*np.log(probabilities+1e-5)))
        entropies = np.sort(np.nan_to_num(entropies))
        plt.plot(range(len(y_true)), entropies)
        plt.title(&#39;Entropies for each input&#39;)
        plt.xlabel(&#39;Sample Index&#39;)
        plt.ylabel(&#39;entropy&#39;)
        self._save(save_path, &#34;entropy&#34;) if save_path else plt.show()
    else:
        raise Exception(&#34;Entropy is only available for classification&#34;)</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.learning_diagnostics"><code class="name flex">
<span>def <span class="ident">learning_diagnostics</span></span>(<span>self, loss_file: str, save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>plots the evolution of the loss function during the training. The losses should be saved inside a file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>loss_file</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the file with the training losses</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def learning_diagnostics(self, loss_file: str, save_path=None):
    &#34;&#34;&#34;plots the evolution of the loss function during the training. The losses should be saved inside a file.

    Args:
        loss_file (str): The path to the file with the training losses
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.
    &#34;&#34;&#34;
    if loss_file != None:
        losses = np.loadtxt(loss_file)
        plt.plot(losses)
        plt.title(&#34;Training Loss&#34;)
        plt.xlabel(&#34;Iterations&#34;)
        plt.ylabel(&#34;Loss&#34;)
        plt.legend()
        self._save(save_path, &#34;learning_diagnostics&#34;) if save_path else plt.show()</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.plot_decision_boundaries"><code class="name flex">
<span>def <span class="ident">plot_decision_boundaries</span></span>(<span>self, dimension=2, granularity=0.01, n_boundaries=30, n_samples=100, data_type='test', un_zoom_level=0.2, save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>plots the n_boundaries decision boundaries for a classification task</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dimension</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the dimension of the feature space for the plot. Defaults to 2.</dd>
<dt><strong><code>granularity</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The precision of the plot.</dd>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>un_zoom_level</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The zoom level for the plot. Defaults to 0.2.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a classification problem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_decision_boundaries(self, dimension=2, granularity=1e-2, n_boundaries=30, n_samples=100,
                             data_type=&#34;test&#34;, un_zoom_level=0.2, save_path=None):
    &#34;&#34;&#34;plots the n_boundaries decision boundaries for a classification task

    Args:
        dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
        granularity (int, optional): The precision of the plot.
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

    Raises:
        ValueError: when the method is called for other than a classification problem

    &#34;&#34;&#34;
    if self._dataset.likelihood_model != &#34;Classification&#34;:
        raise ValueError(&#34;Decision boundary can only be plotted for Classification&#34;)
    x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                       data_type=data_type)
    if dimension == 2:
        self._plot_2d_decision_boundary(x, y, base_matrix, dimension=2, granularity= granularity, 
                                        n_boundaries=10, un_zoom_level=un_zoom_level)
        self._save(save_path, &#34;decision_boundaries&#34;) if save_path else plt.show()
    else:
        raise ValueError(&#34;Decision boundary can only be plotted in 2 dimensions&#34;)</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.plot_uncertainty_area"><code class="name flex">
<span>def <span class="ident">plot_uncertainty_area</span></span>(<span>self, dimension=2, granularity: float = 0.01, n_samples=100, data_type='test', uncertainty_threshold=0.8, un_zoom_level=0.2, save_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>plots the uncertainty area for a classification task</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dimension</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the dimension of the feature space for the plot. Defaults to 2.</dd>
<dt><strong><code>granularity</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The precision of the plot. Defaults to 1e-2.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>uncertainty_threshold</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the threshold below which we consider the prediction uncertain. Defaults to 0.8.</dd>
<dt><strong><code>un_zoom_level</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The zoom level for the plot. Defaults to 0.2.</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a classification problem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_uncertainty_area(self,
                          dimension=2,
                          granularity: float = 1e-2,
                          n_samples=100, data_type=&#34;test&#34;, uncertainty_threshold=0.8,
                          un_zoom_level=0.2,
                          save_path=None):
    &#34;&#34;&#34;plots the uncertainty area for a classification task

    Args:
        dimension (int, optional): the dimension of the feature space for the plot. Defaults to 2.
        granularity (float, optional): The precision of the plot. Defaults to 1e-2.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        uncertainty_threshold (float, optional): the threshold below which we consider the prediction uncertain. Defaults to 0.8.
        un_zoom_level (float, optional): The zoom level for the plot. Defaults to 0.2.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

    Raises:
        ValueError: when the method is called for other than a classification problem
    &#34;&#34;&#34;
    if self._dataset.likelihood_model != &#34;Classification&#34;:
        raise ValueError(&#34;Uncertainty area can only be plotted for Classification&#34;)
    x, y, base_matrix = self._extract_x_y_from_dataset(dimension=dimension, n_samples=n_samples,
                                                       data_type=data_type)
    if dimension == 2:
        self._plot_2d_uncertainty_area(x, y, base_matrix, granularity, n_samples, uncertainty_threshold,
                                       un_zoom_level)
        self._save(save_path, &#34;uncertainty_area&#34;) if save_path else plt.show()</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.regression_uncertainty"><code class="name flex">
<span>def <span class="ident">regression_uncertainty</span></span>(<span>self, n_boundaries=30, n_samples=100, data_type='test', save_path=None) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Plots the epistemic uncertainty for a regression problem</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.</dd>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
<dt><strong><code>save_path</code></strong> :&ensp;<code>string</code>, optional</dt>
<dd>Path to folder in which a report folder will be created and sotres the plots. Defaults to None.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a regression dataset</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def regression_uncertainty(self, n_boundaries = 30, n_samples = 100, data_type=&#34;test&#34;, save_path=None) -&gt; tuple:
    &#34;&#34;&#34;Plots the epistemic uncertainty for a regression problem

    Args:
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation each one gives a new decision boundary. Defaults to 30.
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.
        save_path (string, optional): Path to folder in which a report folder will be created and sotres the plots. Defaults to None.

    Raises:
        ValueError: when the method is called for other than a regression dataset

    &#34;&#34;&#34;
    if self._dataset.likelihood_model == &#34;Regression&#34;:
        x,y_true = self._get_x_y(n_samples, data_type)
        y_samples, y_pred, y_true,x = self._get_predictions(x, n_boundaries, y_true, data_type)
        variance = np.var(y_samples, axis=0)
        err = np.mean(np.sqrt(variance), axis = 1)
        pred_dev = np.mean((y_pred.numpy()-y_true.numpy()), axis = 1)
        # uncertainty
        plt.figure(figsize=(10, 5))
        plt.hlines([0], 0, len(err))
        plt.plot(range(len(err)), pred_dev-err, label=&#39;Epistemic Lower&#39;, alpha=0.5)
        plt.scatter(range(len(err)), pred_dev, label=&#39;Averaged deviation&#39;, alpha=0.5, c=&#34;k&#34;)
        plt.plot(range(len(err)), pred_dev+err, label=&#39;Epistemic Upper&#39;, alpha=0.5)
        plt.legend()
        plt.title(&#39;Epistemic Uncertainty&#39;)
        plt.ylabel(&#39;Pred-True difference&#39;)
        self._save(save_path, &#34;epistemic_uncertainty&#34;) if save_path else plt.show()
    else:
        raise ValueError(&#34;regression uncertainty cannot be computed for other than regression problems&#34;)</code></pre>
</details>
</dd>
<dt id="Pyesian.visualisations.Plotter.Plotter.roc_one_vs_rest"><code class="name flex">
<span>def <span class="ident">roc_one_vs_rest</span></span>(<span>self, n_samples=100, label_of_interest: int = 0, n_boundaries=10, data_type='test')</span>
</code></dt>
<dd>
<div class="desc"><p>plots the ROC curve in a one vs the rest fashion</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>n_samples</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>number of samples from the dataset. Defaults to 100.</dd>
<dt><strong><code>label_of_interest</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>The label that will be opposed to the other for ROC calculation. Defaults to 0.</dd>
<dt><strong><code>n_boundaries</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of sampled networks for the Monte Carlo approximation. Defaults to 10.</dd>
<dt><strong><code>data_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>the split of the dataset on which to calculate the metrics. Defaults to "test".</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>when the method is called for other than a classification problem</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def roc_one_vs_rest(self, n_samples = 100, label_of_interest: int = 0, n_boundaries = 10, data_type = &#34;test&#34;):
    &#34;&#34;&#34;plots the ROC curve in a one vs the rest fashion

    Args:
        n_samples (int, optional): number of samples from the dataset. Defaults to 100.
        label_of_interest (int, optional): The label that will be opposed to the other for ROC calculation. Defaults to 0.
        n_boundaries (int, optional): the number of sampled networks for the Monte Carlo approximation. Defaults to 10.
        data_type (str, optional): the split of the dataset on which to calculate the metrics. Defaults to &#34;test&#34;.

    Raises:
        ValueError: when the method is called for other than a classification problem
    &#34;&#34;&#34;
    if self._dataset.likelihood_model != &#34;Classification&#34;:
        raise ValueError(&#34;ROC can only be plotted for Classification&#34;)  
    x,y_true = self._get_x_y(n_samples, data_type)
    y_samples, y_pred, y_true, x = self._get_predictions(x, n_boundaries, y_true, data_type)
    one_hot_y_true = tf.one_hot(y_true, y_pred.shape[1])
    display = sk.metrics.RocCurveDisplay.from_predictions(
        one_hot_y_true[:, label_of_interest],
        y_pred[:, label_of_interest],
        name=f&#34;class {label_of_interest} vs the rest&#34;,
        color=&#34;blue&#34;,
        plot_chance_level=True,
    )
    _ = display.ax_.set(
        xlabel=&#34;False Positive Rate&#34;,
        ylabel=&#34;True Positive Rate&#34;,
        title=&#34;ROC curve One-vs-Rest&#34;,
    )
    plt.show()</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Pyesian.visualisations" href="index.html">Pyesian.visualisations</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Pyesian.visualisations.Plotter.Plotter" href="#Pyesian.visualisations.Plotter.Plotter">Plotter</a></code></h4>
<ul class="">
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.compare_prediction_to_target" href="#Pyesian.visualisations.Plotter.Plotter.compare_prediction_to_target">compare_prediction_to_target</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.confusion_matrix" href="#Pyesian.visualisations.Plotter.Plotter.confusion_matrix">confusion_matrix</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.entropy" href="#Pyesian.visualisations.Plotter.Plotter.entropy">entropy</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.learning_diagnostics" href="#Pyesian.visualisations.Plotter.Plotter.learning_diagnostics">learning_diagnostics</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.plot_decision_boundaries" href="#Pyesian.visualisations.Plotter.Plotter.plot_decision_boundaries">plot_decision_boundaries</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.plot_uncertainty_area" href="#Pyesian.visualisations.Plotter.Plotter.plot_uncertainty_area">plot_uncertainty_area</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.regression_uncertainty" href="#Pyesian.visualisations.Plotter.Plotter.regression_uncertainty">regression_uncertainty</a></code></li>
<li><code><a title="Pyesian.visualisations.Plotter.Plotter.roc_one_vs_rest" href="#Pyesian.visualisations.Plotter.Plotter.roc_one_vs_rest">roc_one_vs_rest</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>